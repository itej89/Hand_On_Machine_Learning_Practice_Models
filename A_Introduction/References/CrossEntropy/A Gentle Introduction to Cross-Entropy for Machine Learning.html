<!DOCTYPE html>
<html class=" js csstransforms3d csstransitions" style="min-height: 100%;" lang="en-US"><head>
<meta charset="UTF-8">

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="pingback" href="https://machinelearningmastery.com/xmlrpc.php">

<!--  Mobile viewport scale -->
<meta content="initial-scale=1.0, maximum-scale=1.0, user-scalable=yes" name="viewport">

	<!-- This site is optimized with the Yoast SEO plugin v15.9.1 - https://yoast.com/wordpress/plugins/seo/ -->
	<title>A Gentle Introduction to Cross-Entropy for Machine Learning</title><link rel="stylesheet" href="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/90c8e151f7a73f64eb67dc631568d25d.css" media="all" data-minify="1">
	<meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
	<link rel="canonical" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">
	<meta property="og:locale" content="en_US">
	<meta property="og:type" content="article">
	<meta property="og:title" content="A Gentle Introduction to Cross-Entropy for Machine Learning">
	<meta property="og:description" content="Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy […]">
	<meta property="og:url" content="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">
	<meta property="og:site_name" content="Machine Learning Mastery">
	<meta property="article:publisher" content="https://www.facebook.com/MachineLearningMastery/">
	<meta property="article:author" content="https://www.facebook.com/MachineLearningMastery/">
	<meta property="article:published_time" content="2019-10-20T18:00:36+00:00">
	<meta property="article:modified_time" content="2020-12-22T02:49:08+00:00">
	<meta property="og:image" content="https://machinelearningmastery.com/wp-content/uploads/2019/10/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-a-Binary-Classification-Task-With-Extreme-Case-Removed3.png">
	<meta property="og:image:width" content="1280">
	<meta property="og:image:height" content="960">
	<meta name="twitter:label1" content="Written by">
	<meta name="twitter:data1" content="Jason Brownlee">
	<meta name="twitter:label2" content="Est. reading time">
	<meta name="twitter:data2" content="30 minutes">
	<script id="twitter-wjs" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/widgets.js"></script><script id="twitcount_plugins" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/twitcount.js"></script><script type="text/javascript" async="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/9556588.js"></script><script type="application/ld+json" class="yoast-schema-graph">{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://machinelearningmastery.com/#organization","name":"Machine Learning Mastery","url":"https://machinelearningmastery.com/","sameAs":["https://www.facebook.com/MachineLearningMastery/","https://www.linkedin.com/company/machine-learning-mastery","https://twitter.com/TeachTheMachine"],"logo":{"@type":"ImageObject","@id":"https://machinelearningmastery.com/#logo","inLanguage":"en-US","url":"https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon.png","width":512,"height":512,"caption":"Machine Learning Mastery"},"image":{"@id":"https://machinelearningmastery.com/#logo"}},{"@type":"WebSite","@id":"https://machinelearningmastery.com/#website","url":"https://machinelearningmastery.com/","name":"Machine Learning Mastery","description":"Making developers awesome at machine learning","publisher":{"@id":"https://machinelearningmastery.com/#organization"},"potentialAction":[{"@type":"SearchAction","target":"https://machinelearningmastery.com/?s={search_term_string}","query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"ImageObject","@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#primaryimage","inLanguage":"en-US","url":"https://machinelearningmastery.com/wp-content/uploads/2019/10/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-a-Binary-Classification-Task-With-Extreme-Case-Removed3.png","width":1280,"height":960,"caption":"Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task With Extreme Case Removed"},{"@type":"WebPage","@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#webpage","url":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/","name":"A Gentle Introduction to Cross-Entropy for Machine Learning","isPartOf":{"@id":"https://machinelearningmastery.com/#website"},"primaryImageOfPage":{"@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#primaryimage"},"datePublished":"2019-10-20T18:00:36+00:00","dateModified":"2020-12-22T02:49:08+00:00","breadcrumb":{"@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://machinelearningmastery.com/cross-entropy-for-machine-learning/"]}]},{"@type":"BreadcrumbList","@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://machinelearningmastery.com/","url":"https://machinelearningmastery.com/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://machinelearningmastery.com/blog/","url":"https://machinelearningmastery.com/blog/","name":"Blog"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/","url":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/","name":"A Gentle Introduction to Cross-Entropy for Machine Learning"}}]},{"@type":"Article","@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#article","isPartOf":{"@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#webpage"},"author":{"@id":"https://machinelearningmastery.com/#/schema/person/e2d0ff4828d406a3b47e5a3c9a0591e8"},"headline":"A Gentle Introduction to Cross-Entropy for Machine Learning","datePublished":"2019-10-20T18:00:36+00:00","dateModified":"2020-12-22T02:49:08+00:00","mainEntityOfPage":{"@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#webpage"},"commentCount":49,"publisher":{"@id":"https://machinelearningmastery.com/#organization"},"image":{"@id":"https://machinelearningmastery.com/cross-entropy-for-machine-learning/#primaryimage"},"articleSection":"Probability","inLanguage":"en-US","potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https://machinelearningmastery.com/cross-entropy-for-machine-learning/#respond"]}]},{"@type":"Person","@id":"https://machinelearningmastery.com/#/schema/person/e2d0ff4828d406a3b47e5a3c9a0591e8","name":"Jason Brownlee","image":{"@type":"ImageObject","@id":"https://machinelearningmastery.com/#personlogo","inLanguage":"en-US","url":"https://secure.gravatar.com/avatar/a0942b56b07831ac15d4a168a750e34a?s=96&d=mm&r=g","caption":"Jason Brownlee"},"description":"Jason Brownlee, PhD is a machine learning specialist who teaches developers how to get results with modern machine learning methods via hands-on tutorials.","sameAs":["http://MachineLearningMastery.com","https://www.facebook.com/MachineLearningMastery/","https://www.linkedin.com/company/machine-learning-mastery","https://twitter.com/teachthemachine"]}]}</script>
	<!-- / Yoast SEO plugin. -->


<link rel="dns-prefetch" href="https://ads.adthrive.com/">
<link rel="dns-prefetch" href="https://www.google-analytics.com/">
<link rel="dns-prefetch" href="https://loadeu.exelator.com/">
<link rel="dns-prefetch" href="https://sync.crwdcntrl.net/">
<link rel="dns-prefetch" href="https://gdpr-wrapper.privacymanager.io/">
<link rel="dns-prefetch" href="https://securepubads.g.doubleclick.net/">
<link rel="dns-prefetch" href="https://gdpr.privacymanager.io/">
<link rel="dns-prefetch" href="https://sb.scorecardresearch.com/">
<link rel="dns-prefetch" href="https://confiant-integrations.global.ssl.fastly.net/">
<link href="https://fonts.gstatic.com/" crossorigin="" rel="preconnect">
<link rel="alternate" type="application/rss+xml" title="Machine Learning Mastery » Feed" href="https://feeds.feedburner.com/MachineLearningMastery">
<link rel="alternate" type="application/rss+xml" title="Machine Learning Mastery » Comments Feed" href="https://machinelearningmastery.com/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="Machine Learning Mastery » A Gentle Introduction to Cross-Entropy for Machine Learning Comments Feed" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/feed/">
<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	









<!--[if lt IE 9]>
<link href="https://machinelearningmastery.com/wp-content/themes/canvas-new/css/non-responsive.css" rel="stylesheet" type="text/css" />
<style type="text/css">.col-full, #wrapper { width: 960px; max-width: 960px; } #inner-wrapper { padding: 0; } body.full-width #header, #nav-container, body.full-width #content, body.full-width #footer-widgets, body.full-width #footer { padding-left: 0; padding-right: 0; } body.fixed-mobile #top, body.fixed-mobile #header-container, body.fixed-mobile #footer-container, body.fixed-mobile #nav-container, body.fixed-mobile #footer-widgets-container { min-width: 960px; padding: 0 1em; } body.full-width #content { width: auto; padding: 0 1em;}</style>
<![endif]-->
<script type="text/javascript" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/jquery.js" id="jquery-core-js" defer="defer"></script>



<script type="text/javascript" id="ssb-front-js-js-extra">
/* <![CDATA[ */
var SSB = {"ajax_url":"https:\/\/machinelearningmastery.com\/wp-admin\/admin-ajax.php","fb_share_nonce":"9a0a206855"};
/* ]]> */
</script>




<link rel="https://api.w.org/" href="https://machinelearningmastery.com/wp-json/"><link rel="alternate" type="application/json" href="https://machinelearningmastery.com/wp-json/wp/v2/posts/8877"><link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://machinelearningmastery.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://machinelearningmastery.com/wp-includes/wlwmanifest.xml"> 
<link rel="shortlink" href="https://machinelearningmastery.com/?p=8877">
 <style media="screen">

		.simplesocialbuttons.simplesocialbuttons_inline .ssb-fb-like {
	  margin: ;
	}
		 /*inline margin*/
	
	
	
	
	
	
			 .simplesocialbuttons.simplesocialbuttons_inline.simplesocial-simple-icons button{
		 margin: ;
	 }

			 /*margin-digbar*/

	
	
	
	
	
	
	
</style>

<!-- Open Graph Meta Tags generated by Simple Social Buttons 3.2.1 -->
<meta property="og:title" content="A Gentle Introduction to Cross-Entropy for Machine Learning - Machine Learning Mastery">
<meta property="og:description" content="Cross-entropy is commonly used in machine learning as a loss function.

Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the">
<meta property="og:url" content="https://machinelearningmastery.com/cross-entropy-for-machine-learning/">
<meta property="og:site_name" content="Machine Learning Mastery">
<meta property="og:image" content="https://machinelearningmastery.com/wp-content/uploads/2019/10/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-a-Binary-Classification-Task-With-Extreme-Case-Removed3.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:description" content="Cross-entropy is commonly used in machine learning as a loss function.

Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the">
<meta name="twitter:title" content="A Gentle Introduction to Cross-Entropy for Machine Learning - Machine Learning Mastery">
<meta property="twitter:image" content="https://machinelearningmastery.com/wp-content/uploads/2019/10/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-a-Binary-Classification-Task-With-Extreme-Case-Removed3.png">
    
<style id="wplmi-inline-css" type="text/css"> span.wplmi-user-avatar { width: 16px;display: inline-block !important;flex-shrink: 0; } img.wplmi-elementor-avatar { border-radius: 100%;margin-right: 3px; } 

</style>

<link rel="preload" as="font" href="https://machinelearningmastery.com/wp-content/themes/canvas-new/includes/fonts/fontawesome-webfont.woff2?v=4.5.0" crossorigin="">
<!-- Custom CSS Styling -->
<style type="text/css">
#logo .site-title, #logo .site-description { display:none; }
body {background-repeat:no-repeat;background-position:top left;background-attachment:scroll;border-top:0px solid #000000;}
#header {background-repeat:no-repeat;background-position:left top;margin-top:0px;margin-bottom:0px;padding-top:10px;padding-bottom:10px;border:0px solid ;}
#logo .site-title a {font:bold 40px/1em "Helvetica Neue", Helvetica, sans-serif;color:#222222;}
#logo .site-description {font:normal 13px/1em "Helvetica Neue", Helvetica, sans-serif;color:#999999;}
body, p { font:normal 14px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#555555; }
h1 { font:bold 28px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }h2 { font:bold 24px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }h3 { font:bold 20px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }h4 { font:bold 16px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }h5 { font:bold 14px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }h6 { font:bold 12px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#222222; }
.page-title, .post .title, .page .title {font:bold 28px/1.1em "Helvetica Neue", Helvetica, sans-serif;color:#222222;}
.post .title a:link, .post .title a:visited, .page .title a:link, .page .title a:visited {color:#222222}
.post-meta { font:normal 12px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#999999; }
.entry, .entry p{ font:normal 15px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#555555; }
.post-more {font:normal 13px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:;border-top:0px solid #e6e6e6;border-bottom:0px solid #e6e6e6;}
#post-author, #connect {border-top:1px solid #e6e6e6;border-bottom:1px solid #e6e6e6;border-left:1px solid #e6e6e6;border-right:1px solid #e6e6e6;border-radius:5px;-moz-border-radius:5px;-webkit-border-radius:5px;background-color:#fafafa}
.nav-entries a, .woo-pagination { font:normal 13px/1em "Helvetica Neue", Helvetica, sans-serif;color:#888; }
.woo-pagination a, .woo-pagination a:hover {color:#888!important}
.widget h3 {font:bold 14px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#555555;border-bottom:1px solid #e6e6e6;}
.widget_recent_comments li, #twitter li { border-color: #e6e6e6;}
.widget p, .widget .textwidget { font:normal 13px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#555555; }
.widget {font:normal 13px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#555555;border-radius:0px;-moz-border-radius:0px;-webkit-border-radius:0px;}
#tabs .inside li a, .widget_woodojo_tabs .tabbable .tab-pane li a { font:bold 12px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#555555; }
#tabs .inside li span.meta, .widget_woodojo_tabs .tabbable .tab-pane li span.meta { font:300 11px/1.5em "Helvetica Neue", Helvetica, sans-serif;color:#999999; }
#tabs ul.wooTabs li a, .widget_woodojo_tabs .tabbable .nav-tabs li a { font:300 11px/2em "Helvetica Neue", Helvetica, sans-serif;color:#999999; }
@media only screen and (min-width:768px) {
ul.nav li a, #navigation ul.rss a, #navigation ul.cart a.cart-contents, #navigation .cart-contents #navigation ul.rss, #navigation ul.nav-search, #navigation ul.nav-search a { font:bold 15px/1.2em "Helvetica Neue", Helvetica, sans-serif;color:#ffffff; } #navigation ul.rss li a:before, #navigation ul.nav-search a.search-contents:before { color:#ffffff;}
#navigation ul.nav > li a:hover, #navigation ul.nav > li:hover a, #navigation ul.nav li ul li a, #navigation ul.cart > li:hover > a, #navigation ul.cart > li > ul > div, #navigation ul.cart > li > ul > div p, #navigation ul.cart > li > ul span, #navigation ul.cart .cart_list a, #navigation ul.nav li.current_page_item a, #navigation ul.nav li.current_page_parent a, #navigation ul.nav li.current-menu-ancestor a, #navigation ul.nav li.current-cat a, #navigation ul.nav li.current-menu-item a { color:#eeeeee!important; }
#navigation ul.nav > li a:hover, #navigation ul.nav > li:hover, #navigation ul.nav li ul, #navigation ul.cart li:hover a.cart-contents, #navigation ul.nav-search li:hover a.search-contents, #navigation ul.nav-search a.search-contents + ul, #navigation ul.cart a.cart-contents + ul, #navigation ul.nav li.current_page_item a, #navigation ul.nav li.current_page_parent a, #navigation ul.nav li.current-menu-ancestor a, #navigation ul.nav li.current-cat a, #navigation ul.nav li.current-menu-item a{background-color:#84abc7!important}
#navigation ul.nav li ul, #navigation ul.cart > li > ul > div  { border: 0px solid #dbdbdb; }
#navigation ul.nav > li:hover > ul  { left: 0; }
#navigation ul.nav > li  { border-right: 0px solid #dbdbdb; }#navigation ul.nav > li:hover > ul  { left: 0; }
#navigation { box-shadow: none; -moz-box-shadow: none; -webkit-box-shadow: none; }#navigation ul li:first-child, #navigation ul li:first-child a { border-radius:0px 0 0 0px; -moz-border-radius:0px 0 0 0px; -webkit-border-radius:0px 0 0 0px; }
#navigation {background:#84abc7;border-top:0px solid #dbdbdb;border-bottom:0px solid #dbdbdb;border-left:0px solid #dbdbdb;border-right:0px solid #dbdbdb;border-radius:0px; -moz-border-radius:0px; -webkit-border-radius:0px;}
#top ul.nav li a { font:normal 12px/1.6em "Helvetica Neue", Helvetica, sans-serif;color:#ddd; }
}
#footer, #footer p { font:normal 13px/1.4em "Helvetica Neue", Helvetica, sans-serif;color:#999999; }
#footer {border-top:1px solid #dbdbdb;border-bottom:0px solid ;border-left:0px solid ;border-right:0px solid ;border-radius:0px; -moz-border-radius:0px; -webkit-border-radius:0px;}
.magazine #loopedSlider .content h2.title a { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
.wooslider-theme-magazine .slide-title a { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
.magazine #loopedSlider .content .excerpt p { font:300 13px/1.5em Arial, sans-serif;color:#cccccc; }
.wooslider-theme-magazine .slide-content p, .wooslider-theme-magazine .slide-excerpt p { font:300 13px/1.5em Arial, sans-serif;color:#cccccc; }
.magazine .block .post .title a {font:bold 18px/1.2em Helvetica Neue, Helvetica, sans-serif;color:#222222; }
#loopedSlider.business-slider .content h2 { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
#loopedSlider.business-slider .content h2.title a { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
.wooslider-theme-business .has-featured-image .slide-title { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
.wooslider-theme-business .has-featured-image .slide-title a { font:bold 24px/1em Arial, sans-serif;color:#ffffff; }
#wrapper #loopedSlider.business-slider .content p { font:300 13px/1.5em Arial, sans-serif;color:#cccccc; }
.wooslider-theme-business .has-featured-image .slide-content p { font:300 13px/1.5em Arial, sans-serif;color:#cccccc; }
.wooslider-theme-business .has-featured-image .slide-excerpt p { font:300 13px/1.5em Arial, sans-serif;color:#cccccc; }
.archive_header { font:bold 18px/1em Arial, sans-serif;color:#222222; }
.archive_header {border-bottom:1px solid #e6e6e6;}
.archive_header .catrss { display:none; }
</style>

<!-- Custom Favicon -->
<link rel="shortcut icon" href="https://machinelearningmastery.com/wp-content/uploads/2019/09/icon-16x16.png">
<!-- Options Panel Custom CSS -->
<style type="text/css">
#logo img {
   max-width: 100%;
   height: auto;
}
</style>


<!-- Woo Shortcodes CSS -->


<!-- Custom Stylesheet -->


<!-- Theme version -->
<meta name="generator" content="Canvas 5.9.21">
<meta name="generator" content="WooFramework 6.2.9">

<link rel="icon" href="https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-32x32.png" sizes="32x32">
<link rel="icon" href="https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-192x192.png" sizes="192x192">
<link rel="apple-touch-icon" href="https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-180x180.png">
<meta name="msapplication-TileImage" content="https://machinelearningmastery.com/wp-content/uploads/2016/09/cropped-icon-270x270.png">
		<style type="text/css" id="wp-custom-css">
			.display-posts-listing.image-left .listing-item {
	overflow: hidden; 
	margin-bottom: 30px;
	width: 100%;
}

.display-posts-listing.image-left .image {
	float: left;
	margin: 0 10px 0 0;
}

.display-posts-listing.image-left .attachment-thumbnail {
	height: auto; 
	width: auto; 
	max-width: 50px; 
	max-height: 50px;
	border-radius: 50%;
}

.display-posts-listing.image-left .title {
	display: block;
}

.display-posts-listing.image-left .excerpt-dash { 
	display: none; 
}

.display-posts-listing.image-left {
	margin: 0 0 40px 0;
}		</style>
		<noscript><style id="rocket-lazyload-nojs-css">.rll-youtube-player, [data-lazy-src]{display:none !important;}</style></noscript><script src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/can-i-show.html"></script><link rel="stylesheet" type="text/css" href="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/twitcount.css" media="all"><style id="fit-vids-style">.fluid-width-video-wrapper{width:100%;position:relative;padding:0;}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper object,.fluid-width-video-wrapper embed {position:absolute;top:0;left:0;width:100%;height:100%;}</style><link rel="prefetch" href="https://machinelearningmastery.com/divergence-between-probability-distributions/"><link rel="prefetch" href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/"></head>
<body class="post-template-default single single-post postid-8877 single-format-standard chrome alt-style-default two-col-left width-960 two-col-left-960">
<div id="wrapper">

	<div id="inner-wrapper">

	<h3 class="nav-toggle icon"><a href="#navigation">Navigation</a></h3>

	<header id="header" class="col-full">

		<div id="logo">
<a href="https://machinelearningmastery.com/" title="Making developers awesome at machine learning"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Header_smaller_text_better-1.webp" alt="Machine Learning Mastery" width="480" height="80"></a>
<span class="site-title"><a href="https://machinelearningmastery.com/">Machine Learning Mastery</a></span>
<span class="site-description">Making developers awesome at machine learning</span>
</div>
	    <div class="header-widget">
	        <div id="custom_html-62" class="widget_text widget widget_custom_html"><div class="textwidget custom-html-widget"><div style="text-align: right;">
	<a href="https://machinelearningmastery.lpages.co/pfml-mini-course/">Click to Take the FREE Probability Crash-Course</a>
</div></div></div><div id="search-4" class="widget widget_search"><div class="search_main">
    <form method="get" class="searchform" action="https://machinelearningmastery.com/">
        <input type="text" class="field s" name="s" value="Search..." onfocus="if (!window.__cfRLUnblockHandlers) return false; if (this.value == 'Search...') {this.value = '';}" onblur="if (!window.__cfRLUnblockHandlers) return false; if (this.value == '') {this.value = 'Search...';}">
		<input type="hidden" name="post_type" value="post">
        <button type="submit" class="fa fa-search submit" name="submit" value="Search"></button>
    </form>
    <div class="fix"></div>
</div></div>	    </div>
	
	</header>
	<nav id="navigation" class="col-full" role="navigation">

	
	<section class="menus">

		<a href="https://machinelearningmastery.com/" class="nav-home"><span>Home</span></a>

	<h3>Main Menu</h3><ul id="main-nav" class="nav fl"><li id="menu-item-6503" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6503"><a href="https://machinelearningmastery.com/start-here/">Get Started</a></li>
<li id="menu-item-6501" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-6501"><a href="https://machinelearningmastery.com/blog/">Blog</a></li>
<li id="menu-item-6506" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-6506 parent"><a href="#">Topics</a>
<ul class="sub-menu">
	<li id="menu-item-6508" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6508"><a href="https://machinelearningmastery.com/category/deep-learning/">Deep Learning (keras)</a></li>
	<li id="menu-item-7921" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-7921"><a href="https://machinelearningmastery.com/category/deep-learning-for-computer-vision/">Computer Vision</a></li>
	<li id="menu-item-6509" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6509"><a href="https://machinelearningmastery.com/category/deep-learning-time-series/">Neural Net Time Series</a></li>
	<li id="menu-item-6515" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6515"><a href="https://machinelearningmastery.com/category/natural-language-processing/">NLP (Text)</a></li>
	<li id="menu-item-8576" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-8576"><a href="https://machinelearningmastery.com/category/generative-adversarial-networks/">GANs</a></li>
	<li id="menu-item-6511" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6511"><a href="https://machinelearningmastery.com/category/lstm/">LSTMs</a></li>
	<li id="menu-item-7106" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-7106"><a href="https://machinelearningmastery.com/category/better-deep-learning/">Better Deep Learning</a></li>
	<li id="menu-item-6512" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6512"><a href="https://machinelearningmastery.com/category/machine-learning-algorithms/">Intro to Algorithms</a></li>
	<li id="menu-item-6507" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6507"><a href="https://machinelearningmastery.com/category/algorithms-from-scratch/">Code Algorithms</a></li>
	<li id="menu-item-6520" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6520"><a href="https://machinelearningmastery.com/category/time-series/">Intro to Time Series</a></li>
	<li id="menu-item-6516" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6516"><a href="https://machinelearningmastery.com/category/python-machine-learning/">Python (scikit-learn)</a></li>
	<li id="menu-item-11055" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-11055"><a href="https://machinelearningmastery.com/category/ensemble-learning/">Ensemble Learning</a></li>
	<li id="menu-item-9966" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-9966"><a href="https://machinelearningmastery.com/category/imbalanced-classification/">Imbalanced Learning</a></li>
	<li id="menu-item-10674" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-10674"><a href="https://machinelearningmastery.com/category/data-preparation/">Data Preparation</a></li>
	<li id="menu-item-6517" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6517"><a href="https://machinelearningmastery.com/category/r-machine-learning/">R (caret)</a></li>
	<li id="menu-item-6522" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6522"><a href="https://machinelearningmastery.com/category/weka-machine-learning/">Weka (no code)</a></li>
	<li id="menu-item-6510" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6510"><a href="https://machinelearningmastery.com/category/linear-algebra/">Linear Algebra</a></li>
	<li id="menu-item-6519" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6519"><a href="https://machinelearningmastery.com/category/statistical-methods/">Statistics</a></li>
	<li id="menu-item-8989" class="menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-8989"><a href="https://machinelearningmastery.com/category/probability/">Probability</a></li>
	<li id="menu-item-6523" class="menu-item menu-item-type-taxonomy menu-item-object-category menu-item-6523"><a href="https://machinelearningmastery.com/category/xgboost/">XGBoost</a></li>
</ul>
</li>
<li id="menu-item-6502" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6502"><a href="https://machinelearningmastery.com/products/">EBooks</a></li>
<li id="menu-item-6500" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6500"><a href="https://machinelearningmastery.com/faq/">FAQ</a></li>
<li id="menu-item-6504" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6504"><a href="https://machinelearningmastery.com/about/">About</a></li>
<li id="menu-item-6505" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-6505"><a href="https://machinelearningmastery.com/contact/">Contact</a></li>
</ul>	<div class="side-nav">
		</div><!-- /#side-nav -->

	</section><!-- /.menus -->

	<a href="#top" class="nav-close"><span>Return to Content</span></a>

</nav>
       
    <!-- #content Starts -->
	    <div id="content" class="col-full">
    
    	<div id="main-sidebar-container">    

            <!-- #main Starts -->
                        <section id="main">                       
<article class="post-8877 post type-post status-publish format-standard has-post-thumbnail hentry category-probability">
	<header>
	<h1 class="title entry-title">A Gentle Introduction to Cross-Entropy for Machine Learning</h1>	</header>
<div class="post-meta"><span class="small">By</span> <span class="author vcard"><span class="fn"><a href="https://machinelearningmastery.com/author/jasonb/" title="Posts by Jason Brownlee" rel="author">Jason Brownlee</a></span></span> <span class="small">on</span> <abbr class="date time published updated" title="2019-10-21T05:00:36+1100">October 21, 2019</abbr>  <span class="small">in</span> <span class="categories"><a href="https://machinelearningmastery.com/category/probability/" title="View all items in Probability">Probability</a></span>  </div>
	<section class="entry">
<div class="simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-8877 post simplesocialbuttons-inline-no-animation simplesocialbuttons-inline-in">
<button class="ssb_tweet-icon" data-href="https://twitter.com/share?text=A+Gentle+Introduction+to+Cross-Entropy+for+Machine+Learning&amp;url=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="nofollow" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 72 72"><path fill="none" d="M0 0h72v72H0z"></path><path class="icon" fill="#fff" d="M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z"></path></svg></span><i class="simplesocialtxt">Tweet </i></button>
		<button class="ssb_fbshare-icon" target="_blank" data-href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" class="_1pbq" color="#ffffff"><path fill="#ffffff" fill-rule="evenodd" class="icon" d="M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z"></path></svg></span>
						<span class="simplesocialtxt">Share </span> </button>
<button class="ssb_linkedin-icon" data-href="https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"> <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="15px" height="14.1px" viewBox="-301.4 387.5 15 14.1" enable-background="new -301.4 387.5 15 14.1" xml:space="preserve"> <g id="XMLID_398_"> <path id="XMLID_399_" fill="#FFFFFF" d="M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z"></path> <path id="XMLID_400_" fill="#FFFFFF" d="M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z"></path> <path id="XMLID_401_" fill="#FFFFFF" d="M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z"></path> </g> </svg> </span>
						<span class="simplesocialtxt">Share</span> </button>
</div>
<p id="last-modified-info">Last Updated on December 22, 2020</p>
<p>Cross-entropy is commonly used in machine learning as a loss function.</p>
<p>Cross-entropy is a measure from the field of information theory, building upon <a href="https://machinelearningmastery.com/what-is-information-entropy/">entropy</a> and generally calculating the difference between two probability distributions. It is closely related to but is different from <a href="https://machinelearningmastery.com/divergence-between-probability-distributions/">KL divergence</a>
 that calculates the relative entropy between two probability 
distributions, whereas cross-entropy can be thought to calculate the 
total entropy between the distributions.</p>
<p>Cross-entropy is also related to and often confused with <a href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/">logistic loss, called log loss</a>.
 Although the two measures are derived from a different source, when 
used as loss functions for classification models, both measures 
calculate the same quantity and can be used interchangeably.</p>
<p>In this tutorial, you will discover cross-entropy for machine learning.</p>
<p>After completing this tutorial, you will know:</p>
<ul>
<li>How to calculate cross-entropy from scratch and using standard machine learning libraries.</li>
<li>Cross-entropy can be used as a loss function when optimizing 
classification models like logistic regression and artificial neural 
networks.</li>
<li>Cross-entropy is different from KL divergence but can be calculated 
using KL divergence, and is different from log loss but calculates the 
same quantity when used as a loss function.</li>
</ul>
<p><strong>Kick-start your project</strong> with my new book <a href="https://machinelearningmastery.com/probability-for-machine-learning/">Probability for Machine Learning</a>, including <em>step-by-step tutorials</em> and the <em>Python&nbsp;source code</em> files for all examples.</p>
<p>Let’s get started.</p>
<ul>
<li><strong>Update Oct/2019</strong>: Gave an example of cross-entropy 
for identical distributions and updated description for this case 
(thanks Ron U). Added an example of calculating the entropy of the known
 class labels.</li>
<li><strong>Update Nov/2019</strong>: Improved structure and added more explanation of entropy. Added intuition for predicted class probabilities.</li>
<li><strong>Update Dec/2020</strong>: Tweaked the introduction to information and entropy to be clearer.</li>
</ul>
<div id="attachment_8886" style="width: 650px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-8886" loading="lazy" class="size-full wp-image-8886" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/A-Gentle-Introduction-to-Cross-Entropy-for-Machine-Learning.jpg" alt="A Gentle Introduction to Cross-Entropy for Machine Learning" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/A-Gentle-Introduction-to-Cross-Entropy-for-Machine-Learning.jpg 640w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/A-Gentle-Introduction-to-Cross-Entropy-for-Machine-Learning-.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" width="640" height="480"><p id="caption-attachment-8886" class="wp-caption-text">A Gentle Introduction to Cross-Entropy for Machine Learning<br>Photo by <a href="https://www.flickr.com/photos/girolame/3819120378/">Jerome Bon</a>, some rights reserved.</p></div>
<h2>Tutorial Overview</h2>
<p>This tutorial is divided into five parts; they are:</p>
<ol>
<li>What Is Cross-Entropy?</li>
<li>Cross-Entropy Versus KL Divergence</li>
<li>How to Calculate Cross-Entropy
<ol>
<li>Two Discrete Probability Distributions</li>
<li>Calculate Cross-Entropy Between Distributions</li>
<li>Calculate Cross-Entropy Between a Distribution and Itself</li>
<li>Calculate Cross-Entropy Using KL Divergence</li>
</ol>
</li>
<li>Cross-Entropy as a Loss Function
<ol>
<li>Calculate Entropy for Class Labels</li>
<li>Calculate Cross-Entropy Between Class Labels and Probabilities</li>
<li>Calculate Cross-Entropy Using Keras</li>
<li>Intuition for Cross-Entropy on Predicted Probabilities</li>
</ol>
</li>
<li>Cross-Entropy Versus Log Loss
<ol>
<li>Log Loss is the Negative Log Likelihood</li>
<li>Log Loss and Cross Entropy Calculate the Same Thing</li>
</ol>
</li>
</ol>
<h2>What Is Cross-Entropy?</h2>
<p>Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events.</p>
<p>You might recall that <strong>information</strong> quantifies the 
number of bits required to encode and transmit an event. Lower 
probability events have more information, higher probability events have
 less information.</p>
<p>In information theory, we like to describe the “<em>surprise</em>” of an event. An event is more surprising the less likely it is, meaning it contains more information.</p>
<ul>
<li><strong>Low Probability Event</strong> (<em>surprising</em>): More information.</li>
<li><strong>Higher Probability Event</strong> (<em>unsurprising</em>): Less information.</li>
</ul>
<p>Information <em>h(x)</em> can be calculated for an event <em>x</em>, given the probability of the event <em>P(x)</em> as follows:</p>
<ul>
<li>h(x) = -log(P(x))</li>
</ul>
<p><strong>Entropy</strong> is the number of bits required to transmit a
 randomly selected event from a probability distribution. A skewed 
distribution has a low entropy, whereas a distribution where events have
 equal probability has a larger entropy.</p>
<p>A skewed probability distribution has less “surprise” and in turn a 
low entropy because likely events dominate. Balanced distribution are 
more surprising and turn have higher entropy because events are equally 
likely.</p>
<ul>
<li><strong>Skewed Probability Distribution</strong> (<em>unsurprising</em>): Low entropy.</li>
<li><strong>Balanced Probability Distribution</strong> (<em>surprising</em>): High entropy.</li>
</ul>
<p>Entropy <em>H(x)</em> can be calculated for a random variable with a set of <em>x</em> in <em>X</em> discrete states discrete states and their probability <em>P(x)</em> as follows:</p>
<ul>
<li>H(X) = – sum x in X P(x) * log(P(x))</li>
</ul>
<p>If you would like to know more about calculating information for events and entropy for distributions see this tutorial:</p>
<ul>
<li><a href="https://machinelearningmastery.com/what-is-information-entropy/">A Gentle Introduction to Information Entropy</a></li>
</ul>
<p><strong>Cross-entropy</strong> builds upon the idea of entropy from 
information theory and calculates the number of bits required to 
represent or transmit an average event from one distribution compared to
 another distribution.</p>
<blockquote><p>… the cross entropy is the average number of bits needed 
to encode data coming from a source with distribution p when we use 
model q …</p></blockquote>
<p>— Page 57, <a href="https://amzn.to/2xKSTCP">Machine Learning: A Probabilistic Perspective</a>, 2012.</p>
<p>The intuition for this definition comes if we consider a target or 
underlying probability distribution P and an approximation of the target
 distribution Q, then the cross-entropy of Q from P is the number of 
additional bits to represent an event using Q instead of P.</p>
<p>The cross-entropy between two probability distributions, such as Q from P, can be stated formally as:</p>
<ul>
<li>H(P, Q)</li>
</ul>
<p>Where H() is the cross-entropy function, P may be the target distribution and Q is the approximation of the target distribution.</p>
<p>Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows:</p>
<ul>
<li>H(P, Q) = – sum x in X P(x) * log(Q(x))</li>
</ul>
<p>Where P(x) is the probability of the event x in P, Q(x) is the 
probability of event x in Q and log is the base-2 logarithm, meaning 
that the results are in bits. If the base-e or natural logarithm is used
 instead, the result will have the units called nats.</p>
<p>This calculation is for discrete probability distributions, although a
 similar calculation can be used for continuous probability 
distributions using the integral across the events instead of the sum.</p>
<p>The result will be a positive number measured in bits and will be 
equal to the entropy of the distribution if the two probability 
distributions are identical.</p>
<p><strong>Note</strong>: this notation looks a lot like the joint probability, or more specifically, the <a href="https://en.wikipedia.org/wiki/Joint_entropy">joint entropy</a>
 between P and Q. This is misleading as we are scoring the difference 
between probability distributions with cross-entropy. Whereas, joint 
entropy is a different concept that uses the same notation and instead 
calculates the uncertainty across two (or more) random variables.</p>
<div class="woo-sc-hr"></div>
<center>
<h3>Want to Learn Probability for Machine Learning</h3>
<p>Take my free 7-day email crash course now (with sample code).</p>
<p>Click to sign-up and also get a free PDF Ebook version of the course.</p>
<p><a href="https://machinelearningmastery.lpages.co/leadbox/16cf92561172a2%3A164f8be4f346dc/4623731828588544/" target="_blank" style="background: rgb(255, 206, 10); color: rgb(255, 255, 255); text-decoration: none; font-family: Helvetica, Arial, sans-serif; font-weight: bold; font-size: 16px; line-height: 20px; padding: 10px; display: inline-block; max-width: 300px; border-radius: 5px; text-shadow: rgba(0, 0, 0, 0.25) 0px -1px 1px; box-shadow: rgba(255, 255, 255, 0.5) 0px 1px 3px inset, rgba(0, 0, 0, 0.5) 0px 1px 3px;" rel="noopener noreferrer" data-leadbox="https://machinelearningmastery.lpages.co/leadbox/16cf92561172a2%3A164f8be4f346dc/4623731828588544/" data-leadbox-id="16cf92561172a2:164f8be4f346dc">Download Your FREE Mini-Course</a></p>
</center>
<div class="woo-sc-hr"></div>
<h2>Cross-Entropy Versus KL Divergence</h2>
<p>Cross-entropy is not KL Divergence.</p>
<p>Cross-entropy is related to divergence measures, such as the <a href="https://machinelearningmastery.com/divergence-between-probability-distributions/">Kullback-Leibler, or KL, Divergence</a> that quantifies how much one distribution differs from another.</p>
<p>Specifically, the KL divergence measures a very similar quantity to 
cross-entropy. It measures the average number of extra bits required to 
represent a message with Q instead of P, not the total number of bits.</p>
<blockquote><p>In other words, the KL divergence is the average number 
of extra bits needed to encode the data, due to the fact that we used 
distribution q to encode the data instead of the true distribution p.</p></blockquote>
<p>— Page 58, <a href="https://amzn.to/2xKSTCP">Machine Learning: A Probabilistic Perspective</a>, 2012.</p>
<p>As such, the KL divergence is often referred to as the “<em>relative entropy</em>.”</p>
<ul>
<li><strong>Cross-Entropy</strong>: Average number of total bits to represent an event from Q instead of P.</li>
<li><strong>Relative Entropy</strong> (<em>KL Divergence</em>): Average number of extra bits to represent an event from Q instead of P.</li>
</ul>
<p>KL divergence can be calculated as the negative sum of probability of
 each event in P multiples by the log of the probability of the event in
 Q over the probability of the event in P. Typically, log base-2 so that
 the result is measured in bits.</p>
<ul>
<li>KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))</li>
</ul>
<p>The value within the sum is the divergence for a given event.</p>
<p>As such, we can calculate the cross-entropy by adding the entropy of 
the distribution plus the additional entropy calculated by the KL 
divergence. This is intuitive, given the definition of both 
calculations; for example:</p>
<ul>
<li>H(P, Q) = H(P) + KL(P || Q)</li>
</ul>
<p>Where H(P, Q) is the cross-entropy of Q from P, H(P) is the entropy of P and KL(P || Q) is the divergence of Q from P.</p>
<p>Entropy can be calculated for a probability distribution as the 
negative sum of the probability for each event multiplied by the log of 
the probability for the event, where log is base-2 to ensure the result 
is in bits.</p>
<ul>
<li>H(P) = – sum x on X p(x) * log(p(x))</li>
</ul>
<p>Like KL divergence, cross-entropy is not symmetrical, meaning that:</p>
<ul>
<li>H(P, Q) != H(Q, P)</li>
</ul>
<p>As we will see later, both cross-entropy and KL divergence calculate 
the same quantity when they are used as loss functions for optimizing a 
classification predictive model. It is under this context that you might
 sometimes see that cross-entropy and KL divergence are the same.</p>
<p>For a lot more detail on the KL Divergence, see the tutorial:</p>
<ul>
<li><a href="https://machinelearningmastery.com/divergence-between-probability-distributions/">How to Calculate the KL Divergence for Machine Learning</a></li>
</ul>
<h2>How to Calculate Cross-Entropy</h2>
<p>In this section we will make the calculation of cross-entropy concrete with a small example.</p>
<h3>Two Discrete Probability Distributions</h3>
<p>Consider a random variable with three discrete events as different colors: red, green, and blue.</p>
<p>We may have two different probability distributions for this variable; for example:</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1ddff302492333" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">...
# define distributions
events = ['red', 'green', 'blue']
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1ddff302492333-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1ddff302492333-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1ddff302492333-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1ddff302492333-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1ddff302492333-5">5</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1ddff302492333-1"><span class="crayon-sy">.</span><span class="crayon-sy">.</span><span class="crayon-sy">.</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1ddff302492333-2"><span class="crayon-p"># define distributions</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1ddff302492333-3"><span class="crayon-v">events</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-s">'red'</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">'green'</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">'blue'</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1ddff302492333-4"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1ddff302492333-5"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>We can plot a bar chart of these probabilities to compare them directly as probability histograms.</p>
<p>The complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de04268596775" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># plot of distributions
from matplotlib import pyplot
# define distributions
events = ['red', 'green', 'blue']
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]
print('P=%.3f Q=%.3f' % (sum(p), sum(q)))
# plot first distribution
pyplot.subplot(2,1,1)
pyplot.bar(events, p)
# plot second distribution
pyplot.subplot(2,1,2)
pyplot.bar(events, q)
# show the plot
pyplot.show()</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de04268596775-15">15</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-1"><span class="crayon-p"># plot of distributions</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-2"><span class="crayon-e">from </span><span class="crayon-e">matplotlib </span><span class="crayon-e">import </span><span class="crayon-i">pyplot</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-3"><span class="crayon-p"># define distributions</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-4"><span class="crayon-v">events</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-s">'red'</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">'green'</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-s">'blue'</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-5"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-6"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-7"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'P=%.3f Q=%.3f'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-sy">(</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-8"><span class="crayon-p"># plot first distribution</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-9"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">subplot</span><span class="crayon-sy">(</span><span class="crayon-cn">2</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-10"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">bar</span><span class="crayon-sy">(</span><span class="crayon-v">events</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-11"><span class="crayon-p"># plot second distribution</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-12"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">subplot</span><span class="crayon-sy">(</span><span class="crayon-cn">2</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-cn">2</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-13"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">bar</span><span class="crayon-sy">(</span><span class="crayon-v">events</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-14"><span class="crayon-p"># show the plot</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de04268596775-15"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">show</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example creates a histogram for each probability 
distribution, allowing the probabilities for each event to be directly 
compared.</p>
<p>We can see that indeed the distributions are different.</p>
<div id="attachment_8884" style="width: 1290px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-8884" loading="lazy" class="size-full wp-image-8884" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-fo_002.webp" alt="Histogram of Two Different Probability Distributions for the Same Random Variable" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-fo_002.webp 1280w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-for-the.png 300w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-for_003.png 768w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-for_002.png 1024w" sizes="(max-width: 1280px) 100vw, 1280px" width="1280" height="960"><p id="caption-attachment-8884" class="wp-caption-text">Histogram of Two Different Probability Distributions for the Same Random Variable</p></div>
<h3>Calculate Cross-Entropy Between Distributions</h3>
<p>Next, we can develop a function to calculate the cross-entropy between the two distributions.</p>
<p>We will use log base-2 to ensure the result has units in bits.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de05785748533" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log2(q[i]) for i in range(len(p))])</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de05785748533-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de05785748533-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de05785748533-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de05785748533-1"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de05785748533-2"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de05785748533-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>We can then use this function to calculate the cross-entropy of P from Q, as well as the reverse, Q from P.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de06224718561" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">...
# calculate cross entropy H(P, Q)
ce_pq = cross_entropy(p, q)
print('H(P, Q): %.3f bits' % ce_pq)
# calculate cross entropy H(Q, P)
ce_qp = cross_entropy(q, p)
print('H(Q, P): %.3f bits' % ce_qp)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de06224718561-7">7</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-1"><span class="crayon-sy">.</span><span class="crayon-sy">.</span><span class="crayon-sy">.</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-2"><span class="crayon-p"># calculate cross entropy H(P, Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-3"><span class="crayon-v">ce_pq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-4"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P, Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_pq</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-5"><span class="crayon-p"># calculate cross entropy H(Q, P)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-6"><span class="crayon-v">ce_qp</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de06224718561-7"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(Q, P): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_qp</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Tying this all together, the complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de07027429051" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># example of calculating cross entropy
from math import log2

# calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log2(q[i]) for i in range(len(p))])

# define data
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]
# calculate cross entropy H(P, Q)
ce_pq = cross_entropy(p, q)
print('H(P, Q): %.3f bits' % ce_pq)
# calculate cross entropy H(Q, P)
ce_qp = cross_entropy(q, p)
print('H(Q, P): %.3f bits' % ce_qp)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de07027429051-16">16</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-1"><span class="crayon-p"># example of calculating cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-i">log2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-3">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-4"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-5"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-6"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-7">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-8"><span class="crayon-p"># define data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-9"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-10"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-11"><span class="crayon-p"># calculate cross entropy H(P, Q)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-12"><span class="crayon-v">ce_pq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-13"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P, Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_pq</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-14"><span class="crayon-p"># calculate cross entropy H(Q, P)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-15"><span class="crayon-v">ce_qp</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de07027429051-16"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(Q, P): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_qp</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example first calculates the cross-entropy of Q from P as just over 3 bits, then P from Q as just under 3 bits.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de08046928935" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; z-index: 4;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P, Q): 3.288 bits
H(Q, P): 2.906 bits</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de08046928935-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de08046928935-2">2</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de08046928935-1">H(P, Q): 3.288 bits</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de08046928935-2">H(Q, P): 2.906 bits</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<h3>Calculate Cross-Entropy Between a Distribution and Itself</h3>
<p>If two probability distributions are the same, then the cross-entropy between them will be the entropy of the distribution.</p>
<p>We can demonstrate this by calculating the cross-entropy of P vs P and Q vs Q.</p>
<p>The complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de09462157036" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># example of calculating cross entropy for identical distributions
from math import log2

# calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log2(q[i]) for i in range(len(p))])

# define data
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]
# calculate cross entropy H(P, P)
ce_pp = cross_entropy(p, p)
print('H(P, P): %.3f bits' % ce_pp)
# calculate cross entropy H(Q, Q)
ce_qq = cross_entropy(q, q)
print('H(Q, Q): %.3f bits' % ce_qq)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de09462157036-16">16</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-1"><span class="crayon-p"># example of calculating cross entropy for identical distributions</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-i">log2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-3">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-4"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-5"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-6"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-7">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-8"><span class="crayon-p"># define data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-9"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-10"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-11"><span class="crayon-p"># calculate cross entropy H(P, P)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-12"><span class="crayon-v">ce_pp</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-13"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P, P): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_pp</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-14"><span class="crayon-p"># calculate cross entropy H(Q, Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-15"><span class="crayon-v">ce_qq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de09462157036-16"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(Q, Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_qq</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example first calculates the cross-entropy of Q 
vs Q which is calculated as the entropy for Q, and P vs P which is 
calculated as the entropy for P.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0a235600111" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; z-index: 4;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P, P): 1.361 bits
H(Q, Q): 0.884 bits</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0a235600111-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0a235600111-2">2</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0a235600111-1">H(P, P): 1.361 bits</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0a235600111-2">H(Q, Q): 0.884 bits</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<h3>Calculate Cross-Entropy Using KL Divergence</h3>
<p>We can also calculate the cross-entropy using the KL divergence.</p>
<p>The cross-entropy calculated with KL divergence should be identical, 
and it may be interesting to calculate the KL divergence between the 
distributions as well to see the relative entropy or additional bits 
required instead of the total bits calculated by the cross-entropy.</p>
<p>First, we can define a function to calculate the KL divergence 
between the distributions using log base-2 to ensure the result is also 
in bits.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0b491495152" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate the kl divergence KL(P || Q)
def kl_divergence(p, q):
	return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0b491495152-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0b491495152-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0b491495152-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0b491495152-1"><span class="crayon-p"># calculate the kl divergence KL(P || Q)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0b491495152-2"><span class="crayon-e">def </span><span class="crayon-e">kl_divergence</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0b491495152-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">/</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Next, we can define a function to calculate the entropy for a given probability distribution.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0c706817043" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate entropy H(P)
def entropy(p):
	return -sum([p[i] * log2(p[i]) for i in range(len(p))])</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0c706817043-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0c706817043-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0c706817043-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0c706817043-1"><span class="crayon-p"># calculate entropy H(P)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0c706817043-2"><span class="crayon-e">def </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0c706817043-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Finally, we can calculate the cross-entropy using the <em>entropy()</em> and <em>kl_divergence()</em> functions.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0d900299819" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy H(P, Q)
def cross_entropy(p, q):
	return entropy(p) + kl_divergence(p, q)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0d900299819-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0d900299819-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0d900299819-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0d900299819-1"><span class="crayon-p"># calculate cross entropy H(P, Q)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0d900299819-2"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0d900299819-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">+</span><span class="crayon-h"> </span><span class="crayon-e">kl_divergence</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>To keep the example simple, we can compare the cross-entropy for H(P, Q) to the KL divergence KL(P || Q) and the entropy H(P).</p>
<p>The complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0e131751617" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># example of calculating cross entropy with kl divergence
from math import log2

# calculate the kl divergence KL(P || Q)
def kl_divergence(p, q):
	return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))

# calculate entropy H(P)
def entropy(p):
	return -sum([p[i] * log2(p[i]) for i in range(len(p))])

# calculate cross entropy H(P, Q)
def cross_entropy(p, q):
	return entropy(p) + kl_divergence(p, q)

# define data
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]
# calculate H(P)
en_p = entropy(p)
print('H(P): %.3f bits' % en_p)
# calculate kl divergence KL(P || Q)
kl_pq = kl_divergence(p, q)
print('KL(P || Q): %.3f bits' % kl_pq)
# calculate cross entropy H(P, Q)
ce_pq = cross_entropy(p, q)
print('H(P, Q): %.3f bits' % ce_pq)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-16">16</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-17">17</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-18">18</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-19">19</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-20">20</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-21">21</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-22">22</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-23">23</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-24">24</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-25">25</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-26">26</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0e131751617-27">27</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-1"><span class="crayon-p"># example of calculating cross entropy with kl divergence</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-i">log2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-3">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-4"><span class="crayon-p"># calculate the kl divergence KL(P || Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-5"><span class="crayon-e">def </span><span class="crayon-e">kl_divergence</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-6"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">/</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-7">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-8"><span class="crayon-p"># calculate entropy H(P)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-9"><span class="crayon-e">def </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-10"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-11">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-12"><span class="crayon-p"># calculate cross entropy H(P, Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-13"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-14"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">+</span><span class="crayon-h"> </span><span class="crayon-e">kl_divergence</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-15">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-16"><span class="crayon-p"># define data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-17"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-18"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-19"><span class="crayon-p"># calculate H(P)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-20"><span class="crayon-v">en_p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-21"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">en_p</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-22"><span class="crayon-p"># calculate kl divergence KL(P || Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-23"><span class="crayon-v">kl_pq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">kl_divergence</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-24"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'KL(P || Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">kl_pq</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-25"><span class="crayon-p"># calculate cross entropy H(P, Q)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-26"><span class="crayon-v">ce_pq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0e131751617-27"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P, Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_pq</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example, we can see that the cross-entropy score 
of 3.288 bits is comprised of the entropy of P 1.361 and the additional 
1.927 bits calculated by the KL divergence.</p>
<p>This is a useful example that clearly illustrates the relationship between all three calculations.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de0f954028650" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P): 1.361 bits
KL(P || Q): 1.927 bits
H(P, Q): 3.288 bits</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0f954028650-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0f954028650-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de0f954028650-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0f954028650-1">H(P): 1.361 bits</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de0f954028650-2">KL(P || Q): 1.927 bits</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de0f954028650-3">H(P, Q): 3.288 bits</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<h2>Cross-Entropy as a Loss Function</h2>
<p>Cross-entropy is widely used as a loss function when optimizing classification models.</p>
<p>Two examples that you may encounter include the logistic regression 
algorithm (a linear classification algorithm), and artificial neural 
networks that can be used for classification tasks.</p>
<blockquote><p>… using the cross-entropy error function instead of the 
sum-of-squares for a classification problem leads to faster training as 
well as improved generalization.</p></blockquote>
<p>— Page 235, <a href="https://amzn.to/2JwHE7I">Pattern Recognition and Machine Learning</a>, 2006.</p>
<p>Classification problems are those that involve one or more input variables and the prediction of a class label.</p>
<p>Classification tasks that have just two labels for the output 
variable are referred to as binary classification problems, whereas 
those problems with more than two labels are referred to as categorical 
or multi-class classification problems.</p>
<ul>
<li><strong>Binary Classification</strong>: Task of predicting one of two class labels for a given example.</li>
<li><strong>Multi-Class Classification</strong>: Task of predicting one of more than two class labels for a given example.</li>
</ul>
<p>We can see that the idea of cross-entropy may be useful for optimizing a classification model.</p>
<p>Each example has a known class label with a probability of 1.0, and a
 probability of 0.0 for all other labels. A model can estimate the 
probability of an example belonging to each class label. Cross-entropy 
can then be used to calculate the difference between the two probability
 distributions.</p>
<p>As such, we can map the classification of one example onto the idea 
of a random variable with a probability distribution as follows:</p>
<ul>
<li><strong>Random Variable</strong>: The example for which we require a predicted class label.</li>
<li><strong>Events</strong>: Each class label that could be predicted.</li>
</ul>
<p>In classification tasks, we know the target probability distribution P
 for an input as the class label 0 or 1 interpreted as probabilities as “<em>impossible</em>” or “<em>certain</em>” respectively. These probabilities have no surprise at all, therefore they have no information content or zero entropy.</p>
<p>Our model seeks to approximate the target probability distribution Q.</p>
<p>In the language of classification, these are the actual and the predicted probabilities, or <em>y</em> and <em>yhat</em>.</p>
<ul>
<li><strong>Expected Probability</strong> (<em>y</em>): The known probability of each class label for an example in the dataset (P).</li>
<li><strong>Predicted Probability</strong> (<em>yhat</em>): The probability of each class label an example predicted by the model (Q).</li>
</ul>
<p>We can, therefore, estimate the cross-entropy for a single prediction
 using the cross-entropy calculation described above; for example.</p>
<ul>
<li>H(P, Q) = – sum x in X P(x) * log(Q(x))</li>
</ul>
<p>Where each <em>x</em> in <em>X</em> is a class label that could be assigned to the example, and <em>P(x)</em> will be 1 for the known label and 0 for all other labels.</p>
<p>The cross-entropy for a single example in a binary classification task can be stated by unrolling the sum operation as follows:</p>
<ul>
<li>H(P, Q) = – (P(class0) * log(Q(class0)) + P(class1) * log(Q(class1)))</li>
</ul>
<p>You may see this form of calculating cross-entropy cited in textbooks.</p>
<p>If there are just two class labels, the probability is modeled as the <a href="https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/">Bernoulli distribution</a>
 for the positive class label. This means that the probability for class
 1 is predicted by the model directly, and the probability for class 0 
is given as one minus the predicted probability, for example:</p>
<ul>
<li><strong>Predicted P(class0)</strong> = 1 – yhat</li>
<li><strong>Predicted P(class1)</strong> = yhat</li>
</ul>
<p>When calculating cross-entropy for classification tasks, the base-e 
or natural logarithm is used. This means that the units are in nats, not
 bits.</p>
<p>We are often interested in minimizing the cross-entropy for the model
 across the entire training dataset. This is calculated by calculating 
the average cross-entropy across all training examples.</p>
<h3>Calculate Entropy for Class Labels</h3>
<p>Recall that when two distributions are identical, the cross-entropy 
between them is equal to the entropy for the probability distribution.</p>
<p>Class labels are encoded using the values 0 and 1 when preparing data for classification tasks.</p>
<p>For example, if a classification problem has three classes, and an 
example has a label for the first class, then the probability 
distribution will be [1, 0, 0]. If an example has a label for the second
 class, it will have a probability distribution for the two events as 
[0, 1, 0]. This is called a <a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/">one hot encoding</a>.</p>
<p>This probability distribution has no information as the outcome is 
certain. We know the class. Therefore the entropy for this variable is 
zero.</p>
<p>This is an important concept and we can demonstrate it with a worked example.</p>
<p>Pretend with have a classification problem with 3 classes, and we 
have one example that belongs to each class. We can represent each 
example as a discrete probability distribution with a 1.0 probability 
for the class to which the example belongs and a 0.0 probability for all
 other classes.</p>
<p>We can calculate the entropy of the probability distribution for each “<em>variable</em>” across the “<em>events</em>“.</p>
<p>The complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de10427106882" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># entropy of examples from a classification task with 3 classes
from math import log2
from numpy import asarray

# calculate entropy
def entropy(p):
	return -sum([p[i] * log2(p[i]) for i in range(len(p))])

# class 1
p = asarray([1,0,0]) + 1e-15
print(entropy(p))
# class 2
p = asarray([0,1,0]) + 1e-15
print(entropy(p))
# class 3
p = asarray([0,0,1]) + 1e-15
print(entropy(p))</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-16">16</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de10427106882-17">17</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-1"><span class="crayon-p"># entropy of examples from a classification task with 3 classes</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-e">log2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-3"><span class="crayon-e">from </span><span class="crayon-e">numpy </span><span class="crayon-e">import </span><span class="crayon-i">asarray</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-4">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-5"><span class="crayon-p"># calculate entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-6"><span class="crayon-e">def </span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-7"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-o">*</span><span class="crayon-h"> </span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-8">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-9"><span class="crayon-p"># class 1</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-10"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">0</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">+</span><span class="crayon-h"> </span><span class="crayon-cn">1e</span><span class="crayon-o">-</span><span class="crayon-cn">15</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-11"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-12"><span class="crayon-p"># class 2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-13"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-cn">0</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">+</span><span class="crayon-h"> </span><span class="crayon-cn">1e</span><span class="crayon-o">-</span><span class="crayon-cn">15</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-14"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-15"><span class="crayon-p"># class 3</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-16"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">+</span><span class="crayon-h"> </span><span class="crayon-cn">1e</span><span class="crayon-o">-</span><span class="crayon-cn">15</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de10427106882-17"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-e">entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example calculates the entropy for each random variable.</p>
<p>We can see that in each case, the entropy is 0.0 (actually a number very close to zero).</p>
<p>Note that we had to add a very small value to the 0.0 values to avoid the <em>log()</em> from blowing up, as we cannot calculate the log of 0.0.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de15515570590" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">9.805612959471341e-14
9.805612959471341e-14
9.805612959471341e-14</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de15515570590-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de15515570590-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de15515570590-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de15515570590-1">9.805612959471341e-14</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de15515570590-2">9.805612959471341e-14</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de15515570590-3">9.805612959471341e-14</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>As such, the entropy of a known class label is always 0.0.</p>
<p>This means that the cross entropy of two distributions (real and 
predicted) that have the same probability distribution for a class 
label, will also always be 0.0.</p>
<p>Recall that when evaluating a model using cross-entropy on a training
 dataset that we average the cross-entropy across all examples in the 
dataset.</p>
<p>Therefore, a cross-entropy of 0.0 when training a model indicates 
that the predicted class probabilities are identical to the 
probabilities in the training dataset, e.g. zero loss.</p>
<p>We could just as easily minimize the KL divergence as a loss function instead of the cross-entropy.</p>
<p>Recall that the KL divergence is the extra bits required to transmit 
one variable compared to another. It is the cross-entropy without the 
entropy of the class label, which we know would be zero anyway.</p>
<p>As such, minimizing the KL divergence and the cross entropy for a classification task are identical.</p>
<blockquote><p>Minimizing this KL divergence corresponds exactly to minimizing the cross-entropy between the distributions.</p></blockquote>
<p>— Page 132, <a href="https://amzn.to/2lnc3vL">Deep Learning</a>, 2016.</p>
<p>In practice, a cross-entropy loss of 0.0 often indicates that the 
model has overfit the training dataset, but that is another story.</p>
<h3>Calculate Cross-Entropy Between Class Labels and Probabilities</h3>
<p>The use of cross-entropy for classification often gives different 
specific names based on the number of classes, mirroring the name of the
 classification task; for example:</p>
<ul>
<li><strong>Binary Cross-Entropy</strong>: Cross-entropy as a loss function for a binary classification task.</li>
<li><strong>Categorical Cross-Entropy</strong>: Cross-entropy as a loss function for a multi-class classification task.</li>
</ul>
<p>We can make the use of cross-entropy as a loss function concrete with a worked example.</p>
<p>Consider a two-class classification task with the following 10 actual class labels (P) and predicted class labels (Q).</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de16558017269" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">...
# define classification data
p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de16558017269-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de16558017269-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de16558017269-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de16558017269-4">4</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de16558017269-1"><span class="crayon-sy">.</span><span class="crayon-sy">.</span><span class="crayon-sy">.</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de16558017269-2"><span class="crayon-p"># define classification data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de16558017269-3"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de16558017269-4"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">]</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>We can enumerate these probabilities and calculate the 
cross-entropy for each using the cross-entropy function developed in the
 previous section using <em>log()</em> (natural logarithm) instead of <em>log2()</em>.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de17294051970" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log(q[i]) for i in range(len(p))])</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de17294051970-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de17294051970-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de17294051970-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de17294051970-1"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de17294051970-2"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de17294051970-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>For each actual and predicted probability, we must convert 
the prediction into a distribution of probabilities across each event, 
in this case, the classes {0, 1} as 1 minus the probability for class 0 
and probability for class 1.</p>
<p>We can then calculate the cross-entropy and repeat the process for all examples.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de18257801027" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">...
# calculate cross entropy for each example
results = list()
for i in range(len(p)):
	# create the distribution for each event {0, 1}
	expected = [1.0 - p[i], p[i]]
	predicted = [1.0 - q[i], q[i]]
	# calculate cross entropy for the two events
	ce = cross_entropy(expected, predicted)
	print('&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats' % (p[i], q[i], ce))
	results.append(ce)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de18257801027-11">11</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-1"><span class="crayon-sy">.</span><span class="crayon-sy">.</span><span class="crayon-sy">.</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-2"><span class="crayon-p"># calculate cross entropy for each example</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-3"><span class="crayon-v">results</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">list</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-4"><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-5"><span class="crayon-h">	</span><span class="crayon-p"># create the distribution for each event {0, 1}</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-6"><span class="crayon-h">	</span><span class="crayon-v">expected</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-7"><span class="crayon-h">	</span><span class="crayon-v">predicted</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-8"><span class="crayon-h">	</span><span class="crayon-p"># calculate cross entropy for the two events</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-9"><span class="crayon-h">	</span><span class="crayon-v">ce</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">expected</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">predicted</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-10"><span class="crayon-h">	</span><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ce</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de18257801027-11"><span class="crayon-h">	</span><span class="crayon-v">results</span><span class="crayon-sy">.</span><span class="crayon-e">append</span><span class="crayon-sy">(</span><span class="crayon-v">ce</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Finally, we can calculate the average cross-entropy across 
the dataset and report it as the cross-entropy loss for the model on the
 dataset.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de19744693681" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">...
# calculate the average cross entropy
mean_ce = mean(results)
print('Average Cross Entropy: %.3f nats' % mean_ce)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de19744693681-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de19744693681-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de19744693681-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de19744693681-4">4</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de19744693681-1"><span class="crayon-sy">.</span><span class="crayon-sy">.</span><span class="crayon-sy">.</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de19744693681-2"><span class="crayon-p"># calculate the average cross entropy</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de19744693681-3"><span class="crayon-v">mean_ce</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">results</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de19744693681-4"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'Average Cross Entropy: %.3f nats'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">mean_ce</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Tying this all together, the complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1a515352681" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy for classification problem
from math import log
from numpy import mean

# calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log(q[i]) for i in range(len(p))])

# define classification data
p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]
# calculate cross entropy for each example
results = list()
for i in range(len(p)):
	# create the distribution for each event {0, 1}
	expected = [1.0 - p[i], p[i]]
	predicted = [1.0 - q[i], q[i]]
	# calculate cross entropy for the two events
	ce = cross_entropy(expected, predicted)
	print('&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats' % (p[i], q[i], ce))
	results.append(ce)

# calculate the average cross entropy
mean_ce = mean(results)
print('Average Cross Entropy: %.3f nats' % mean_ce)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-16">16</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-17">17</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-18">18</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-19">19</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-20">20</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-21">21</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-22">22</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-23">23</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-24">24</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1a515352681-25">25</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-1"><span class="crayon-p"># calculate cross entropy for classification problem</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-e">log</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-3"><span class="crayon-e">from </span><span class="crayon-e">numpy </span><span class="crayon-e">import </span><span class="crayon-i">mean</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-4">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-5"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-6"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-7"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-8">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-9"><span class="crayon-p"># define classification data</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-10"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-11"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-12"><span class="crayon-p"># calculate cross entropy for each example</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-13"><span class="crayon-v">results</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">list</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-14"><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-15"><span class="crayon-h">	</span><span class="crayon-p"># create the distribution for each event {0, 1}</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-16"><span class="crayon-h">	</span><span class="crayon-v">expected</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-17"><span class="crayon-h">	</span><span class="crayon-v">predicted</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-18"><span class="crayon-h">	</span><span class="crayon-p"># calculate cross entropy for the two events</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-19"><span class="crayon-h">	</span><span class="crayon-v">ce</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">expected</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">predicted</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-20"><span class="crayon-h">	</span><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'&gt;[y=%.1f, yhat=%.1f] ce: %.3f nats'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ce</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-21"><span class="crayon-h">	</span><span class="crayon-v">results</span><span class="crayon-sy">.</span><span class="crayon-e">append</span><span class="crayon-sy">(</span><span class="crayon-v">ce</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-22">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-23"><span class="crayon-p"># calculate the average cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-24"><span class="crayon-v">mean_ce</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">mean</span><span class="crayon-sy">(</span><span class="crayon-v">results</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1a515352681-25"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'Average Cross Entropy: %.3f nats'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">mean_ce</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example prints the actual and predicted probabilities for each example and the cross-entropy in nats.</p>
<p>The final average cross-entropy loss across all examples is reported, in this case, as 0.247 nats.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1b229767214" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">&gt;[y=1.0, yhat=0.8] ce: 0.223 nats
&gt;[y=1.0, yhat=0.9] ce: 0.105 nats
&gt;[y=1.0, yhat=0.9] ce: 0.105 nats
&gt;[y=1.0, yhat=0.6] ce: 0.511 nats
&gt;[y=1.0, yhat=0.8] ce: 0.223 nats
&gt;[y=0.0, yhat=0.1] ce: 0.105 nats
&gt;[y=0.0, yhat=0.4] ce: 0.511 nats
&gt;[y=0.0, yhat=0.2] ce: 0.223 nats
&gt;[y=0.0, yhat=0.1] ce: 0.105 nats
&gt;[y=0.0, yhat=0.3] ce: 0.357 nats
Average Cross Entropy: 0.247 nats</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1b229767214-11">11</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-1">&gt;[y=1.0, yhat=0.8] ce: 0.223 nats</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-2">&gt;[y=1.0, yhat=0.9] ce: 0.105 nats</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-3">&gt;[y=1.0, yhat=0.9] ce: 0.105 nats</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-4">&gt;[y=1.0, yhat=0.6] ce: 0.511 nats</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-5">&gt;[y=1.0, yhat=0.8] ce: 0.223 nats</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-6">&gt;[y=0.0, yhat=0.1] ce: 0.105 nats</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-7">&gt;[y=0.0, yhat=0.4] ce: 0.511 nats</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-8">&gt;[y=0.0, yhat=0.2] ce: 0.223 nats</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-9">&gt;[y=0.0, yhat=0.1] ce: 0.105 nats</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-10">&gt;[y=0.0, yhat=0.3] ce: 0.357 nats</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1b229767214-11">Average Cross Entropy: 0.247 nats</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>This is how cross-entropy loss is calculated when optimizing a
 logistic regression model or a neural network model under a 
cross-entropy loss function.</p>
<h3>Calculate Cross-Entropy Using Keras</h3>
<p>We can confirm the same calculation by using the <em>binary_crossentropy()</em> function from the <a href="https://keras.io/losses/">Keras deep learning API</a> to calculate the cross-entropy loss for our small dataset.</p>
<p>The complete example is listed below.</p>
<p><strong>Note</strong>: This example assumes that you have the <a href="https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/">Keras library</a> installed (e.g. version 2.3 or higher) and configured with a backend library such as <a href="https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/">TensorFlow</a> (version 2.0 or higher). If not, you can skip running this example.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1c968812695" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy with keras
from numpy import asarray
from keras import backend
from keras.losses import binary_crossentropy
# prepare classification data
p = asarray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])
q = asarray([0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3])
# convert to keras variables
y_true = backend.variable(p)
y_pred = backend.variable(q)
# calculate the average cross-entropy
mean_ce = backend.eval(binary_crossentropy(y_true, y_pred))
print('Average Cross Entropy: %.3f nats' % mean_ce)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1c968812695-13">13</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-1"><span class="crayon-p"># calculate cross entropy with keras</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-2"><span class="crayon-e">from </span><span class="crayon-e">numpy </span><span class="crayon-e">import </span><span class="crayon-e">asarray</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-3"><span class="crayon-e">from </span><span class="crayon-e">keras </span><span class="crayon-e">import </span><span class="crayon-e">backend</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-4"><span class="crayon-e">from </span><span class="crayon-v">keras</span><span class="crayon-sy">.</span><span class="crayon-e">losses </span><span class="crayon-e">import </span><span class="crayon-v">binary</span><span class="crayon-sy">_</span>crossentropy</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-5"><span class="crayon-p"># prepare classification data</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-6"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-7"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-8"><span class="crayon-p"># convert to keras variables</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-9"><span class="crayon-v">y_true</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">backend</span><span class="crayon-sy">.</span><span class="crayon-e">variable</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-10"><span class="crayon-v">y_pred</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">backend</span><span class="crayon-sy">.</span><span class="crayon-e">variable</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-11"><span class="crayon-p"># calculate the average cross-entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-12"><span class="crayon-v">mean_ce</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-v">backend</span><span class="crayon-sy">.</span><span class="crayon-e">eval</span><span class="crayon-sy">(</span><span class="crayon-e">binary_crossentropy</span><span class="crayon-sy">(</span><span class="crayon-v">y_true</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">y_pred</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1c968812695-13"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'Average Cross Entropy: %.3f nats'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">mean_ce</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example, we can see that the same average cross-entropy loss of 0.247 nats is reported.</p>
<p>This confirms the correct manual calculation of cross-entropy.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1d607675633" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; z-index: 4;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">Average Cross Entropy: 0.247 nats</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1d607675633-1">1</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1d607675633-1">Average Cross Entropy: 0.247 nats</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<h3>Intuition for Cross-Entropy on Predicted Probabilities</h3>
<p>We can further develop the intuition for the cross-entropy for predicted class probabilities.</p>
<p>For example, given that an average cross-entropy loss of 0.0 is a 
perfect model, what do average cross-entropy values greater than zero 
mean exactly?</p>
<p>We can explore this question no a binary classification problem where
 the class labels as 0 and 1.&nbsp;This is a discrete probability 
distribution with two events and a certain probability for one event and
 an impossible probability for the other event.</p>
<p>We can then calculate the cross entropy for different “<em>predicted</em>”
 probability distributions transitioning from a perfect match of the 
target distribution to the exact opposite probability distribution.</p>
<p>We would expect that as the predicted probability distribution 
diverges further from the target distribution that the cross-entropy 
calculated will increase.</p>
<p>The example below implements this and plots the cross-entropy result 
for the predicted probability distribution compared to the target of [0,
 1] for two events as we would see for the cross-entropy in a binary 
classification task.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1e817311759" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># cross-entropy for predicted probability distribution vs label
from math import log
from matplotlib import pyplot

# calculate cross-entropy
def cross_entropy(p, q, ets=1e-15):
	return -sum([p[i]*log(q[i]+ets) for i in range(len(p))])

# define the target distribution for two events
target = [0.0, 1.0]
# define probabilities for the first event
probs = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]
# create probability distributions for the two events
dists = [[1.0 - p, p] for p in probs]
# calculate cross-entropy for each distribution
ents = [cross_entropy(target, d) for d in dists]
# plot probability distribution vs cross-entropy
pyplot.plot([1-p for p in probs], ents, marker='.')
pyplot.title('Probability Distribution vs Cross-Entropy')
pyplot.xticks([1-p for p in probs], ['[%.1f,%.1f]'%(d[0],d[1]) for d in dists], rotation=70)
pyplot.subplots_adjust(bottom=0.2)
pyplot.xlabel('Probability Distribution')
pyplot.ylabel('Cross-Entropy (nats)')
pyplot.show()</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-16">16</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-17">17</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-18">18</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-19">19</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-20">20</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-21">21</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-22">22</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-23">23</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1e817311759-24">24</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-1"><span class="crayon-p"># cross-entropy for predicted probability distribution vs label</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-e">log</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-3"><span class="crayon-e">from </span><span class="crayon-e">matplotlib </span><span class="crayon-e">import </span><span class="crayon-i">pyplot</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-4">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-5"><span class="crayon-p"># calculate cross-entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-6"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ets</span><span class="crayon-o">=</span><span class="crayon-cn">1e</span><span class="crayon-o">-</span><span class="crayon-cn">15</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-7"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">+</span><span class="crayon-v">ets</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-8">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-9"><span class="crayon-p"># define the target distribution for two events</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-10"><span class="crayon-v">target</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1.0</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-11"><span class="crayon-p"># define probabilities for the first event</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-12"><span class="crayon-v">probs</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.7</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.5</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.0</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-13"><span class="crayon-p"># create probability distributions for the two events</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-14"><span class="crayon-v">dists</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-15"><span class="crayon-p"># calculate cross-entropy for each distribution</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-16"><span class="crayon-v">ents</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">target</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">d</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">d</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">dists</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-17"><span class="crayon-p"># plot probability distribution vs cross-entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-18"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">plot</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ents</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">marker</span><span class="crayon-o">=</span><span class="crayon-s">'.'</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-19"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">title</span><span class="crayon-sy">(</span><span class="crayon-s">'Probability Distribution vs Cross-Entropy'</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-20"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">xticks</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-s">'[%.1f,%.1f]'</span><span class="crayon-o">%</span><span class="crayon-sy">(</span><span class="crayon-v">d</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-v">d</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">d</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">dists</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">rotation</span><span class="crayon-o">=</span><span class="crayon-cn">70</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-21"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">subplots_adjust</span><span class="crayon-sy">(</span><span class="crayon-v">bottom</span><span class="crayon-o">=</span><span class="crayon-cn">0.2</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-22"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">xlabel</span><span class="crayon-sy">(</span><span class="crayon-s">'Probability Distribution'</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-23"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">ylabel</span><span class="crayon-sy">(</span><span class="crayon-s">'Cross-Entropy (nats)'</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1e817311759-24"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">show</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example calculates the cross-entropy score for each probability distribution then plots the results as a line plot.</p>
<p>We can see that as expected, cross-entropy starts at 0.0 (far left 
point) when the predicted probability distribution matches the target 
distribution, then steadily increases as the predicted probability 
distribution diverges.</p>
<p>We can also see a dramatic leap in cross-entropy when the predicted 
probability distribution is the exact opposite of the target 
distribution, that is, [1, 0] compared to the target of [0, 1].</p>
<div id="attachment_9517" style="width: 1290px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-9517" loading="lazy" class="size-full wp-image-9517" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-_002.webp" alt="Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-_002.webp 1280w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-a.png 300w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-f_005.png 1024w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-f_006.png 768w" sizes="(max-width: 1280px) 100vw, 1280px" width="1280" height="960"><p id="caption-attachment-9517" class="wp-caption-text">Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task</p></div>
<p>We are not going to have a model that predicts the exact opposite 
probability distribution for all cases on a binary classification task.</p>
<p>As such, we can remove this case and re-calculate the plot.</p>
<p>The updated version of the code is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de1f887532426" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># cross-entropy for predicted probability distribution vs label
from math import log
from matplotlib import pyplot

# calculate cross-entropy
def cross_entropy(p, q, ets=1e-15):
	return -sum([p[i]*log(q[i]+ets) for i in range(len(p))])

# define the target distribution for two events
target = [0.0, 1.0]
# define probabilities for the first event
probs = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
# create probability distributions for the two events
dists = [[1.0 - p, p] for p in probs]
# calculate cross-entropy for each distribution
ents = [cross_entropy(target, d) for d in dists]
# plot probability distribution vs cross-entropy
pyplot.plot([1-p for p in probs], ents, marker='.')
pyplot.title('Probability Distribution vs Cross-Entropy')
pyplot.xticks([1-p for p in probs], ['[%.1f,%.1f]'%(d[0],d[1]) for d in dists], rotation=70)
pyplot.subplots_adjust(bottom=0.2)
pyplot.xlabel('Probability Distribution')
pyplot.ylabel('Cross-Entropy (nats)')
pyplot.show()</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-16">16</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-17">17</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-18">18</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-19">19</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-20">20</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-21">21</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-22">22</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-23">23</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de1f887532426-24">24</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-1"><span class="crayon-p"># cross-entropy for predicted probability distribution vs label</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-e">log</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-3"><span class="crayon-e">from </span><span class="crayon-e">matplotlib </span><span class="crayon-e">import </span><span class="crayon-i">pyplot</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-4">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-5"><span class="crayon-p"># calculate cross-entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-6"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ets</span><span class="crayon-o">=</span><span class="crayon-cn">1e</span><span class="crayon-o">-</span><span class="crayon-cn">15</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-7"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">+</span><span class="crayon-v">ets</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-8">&nbsp;</div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-9"><span class="crayon-p"># define the target distribution for two events</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-10"><span class="crayon-v">target</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1.0</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-11"><span class="crayon-p"># define probabilities for the first event</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-12"><span class="crayon-v">probs</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.7</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.5</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-13"><span class="crayon-p"># create probability distributions for the two events</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-14"><span class="crayon-v">dists</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">1.0</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-15"><span class="crayon-p"># calculate cross-entropy for each distribution</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-16"><span class="crayon-v">ents</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">target</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">d</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">d</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">dists</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-17"><span class="crayon-p"># plot probability distribution vs cross-entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-18"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">plot</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">ents</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">marker</span><span class="crayon-o">=</span><span class="crayon-s">'.'</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-19"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">title</span><span class="crayon-sy">(</span><span class="crayon-s">'Probability Distribution vs Cross-Entropy'</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-20"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">xticks</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">p</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">probs</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-s">'[%.1f,%.1f]'</span><span class="crayon-o">%</span><span class="crayon-sy">(</span><span class="crayon-v">d</span><span class="crayon-sy">[</span><span class="crayon-cn">0</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-v">d</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">d</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">dists</span><span class="crayon-sy">]</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">rotation</span><span class="crayon-o">=</span><span class="crayon-cn">70</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-21"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">subplots_adjust</span><span class="crayon-sy">(</span><span class="crayon-v">bottom</span><span class="crayon-o">=</span><span class="crayon-cn">0.2</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-22"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">xlabel</span><span class="crayon-sy">(</span><span class="crayon-s">'Probability Distribution'</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-23"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">ylabel</span><span class="crayon-sy">(</span><span class="crayon-s">'Cross-Entropy (nats)'</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de1f887532426-24"><span class="crayon-v">pyplot</span><span class="crayon-sy">.</span><span class="crayon-e">show</span><span class="crayon-sy">(</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example gives a much better idea of the 
relationship between the divergence in probability distribution and the 
calculated cross-entropy.</p>
<p>We can see a super-linear relationship where the more the predicted 
probability distribution diverges from the target, the larger the 
increase in cross-entropy.</p>
<div id="attachment_9518" style="width: 1290px" class="wp-caption aligncenter"><img aria-describedby="caption-attachment-9518" loading="lazy" class="size-full wp-image-9518" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-_003.webp" alt="Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task With Extreme Case Removed" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-_003.webp 1280w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-f_003.png 300w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-f_004.png 1024w, A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-f_002.png 768w" sizes="(max-width: 1280px) 100vw, 1280px" width="1280" height="960"><p id="caption-attachment-9518" class="wp-caption-text">Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task With Extreme Case Removed</p></div>
<p>A plot like this can be used as a guide for interpreting the average 
cross-entropy reported for a model for a binary classification dataset.</p>
<p>For example, you can use these cross-entropy values to interpret the 
mean cross-entropy reported by Keras for a neural network model on a 
binary classification task, or a binary classification model in 
scikit-learn evaluated using the logloss metric.</p>
<p>You can use it to answer the general question:</p>
<blockquote><p><strong>What is a good cross-entropy score?</strong></p></blockquote>
<p>If you are working in nats (and you usually are) and you are getting 
mean cross-entropy less than 0.2, you are off to a good start, and less 
than 0.1 or 0.05 is even better.</p>
<p>On the other hand, if you are getting mean cross-entropy greater than
 0.2 or 0.3 you can probably improve, and if you are getting a mean 
cross-entropy greater than 1.0, then something is going on and you’re 
making poor probability predictions on many examples in your dataset.</p>
<p>We can summarise these intuitions for the mean cross-entropy as follows:</p>
<ul>
<li><strong>Cross-Entropy = 0.00</strong>: Perfect probabilities.</li>
<li><strong>Cross-Entropy &lt; 0.02</strong>: Great probabilities.</li>
<li><strong>Cross-Entropy &lt; 0.05</strong>: On the right track.</li>
<li><strong>Cross-Entropy &lt; 0.20</strong>: Fine.</li>
<li><strong>Cross-Entropy &gt; 0.30</strong>: Not great.</li>
<li><strong>Cross-Entropy &gt; 1.00</strong>: Terrible.</li>
<li><strong>Cross-Entropy &gt; 2.00</strong> Something is broken.</li>
</ul>
<p>This listing will provide a useful guide when interpreting a 
cross-entropy (log loss) from your logistic regression model, or your 
artificial neural network model.</p>
<p>You can also calculate separate mean cross-entropy scores per-class 
and help tease out on which classes you’re model has good probabilities,
 and which it might be messing up.</p>
<h2>Cross-Entropy Versus Log Loss</h2>
<p>Cross-Entropy is not Log Loss, but they calculate the same quantity when used as loss functions for classification problems.</p>
<h3>Log Loss is the Negative Log Likelihood</h3>
<p><a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss">Logistic loss</a> refers to the loss function commonly used to optimize a logistic regression model.</p>
<p>It may also be referred to as logarithmic loss (which is confusing) or simply log loss.</p>
<p>Many models are optimized under a probabilistic framework called the <a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/">maximum likelihood estimation</a>, or MLE, that involves finding a set of parameters that best explain the observed data.</p>
<p>This involves selecting a <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a>
 that defines how likely a set of observations (data) are given model 
parameters. When a log likelihood function is used (which is common), it
 is often referred to as optimizing the log likelihood for the model. 
Because it is more common to minimize a function than to maximize it in 
practice, the log likelihood function is inverted by adding a negative 
sign to the front. This transforms it into a Negative Log Likelihood 
function or NLL for short.</p>
<p>In deriving the log likelihood function under a framework of maximum 
likelihood estimation for Bernoulli probability distribution functions 
(two classes), the calculation comes out to be:</p>
<ul>
<li>negative log-likelihood(P, Q) = -(P(class0) * log(Q(class0)) + P(class1) * log(Q(class1)))</li>
</ul>
<p>This quantity can be averaged over all training examples by calculating the average of the log of the likelihood function.</p>
<p>Negative log-likelihood for binary classification problems is often 
shortened to simply “log loss” as the loss function derived for logistic
 regression.</p>
<ul>
<li>log loss = negative log-likelihood, under a Bernoulli probability distribution</li>
</ul>
<p>We can see that the negative log-likelihood is the same calculation 
as is used for the cross-entropy for Bernoulli probability distribution 
functions (two events or classes). In fact, the negative log-likelihood 
for Multinoulli distributions (multi-class classification) also matches 
the calculation for cross-entropy.</p>
<p>For more on log loss and the negative log likelihood, see the tutorial:</p>
<ul>
<li><a href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/">A Gentle Introduction to Logistic Regression With Maximum Likelihood Estimation</a></li>
</ul>
<h3>Log Loss and Cross Entropy Calculate the Same Thing</h3>
<p>For classification problems, “<em>log loss</em>“, “<em>cross-entropy</em>” and “<em>negative log-likelihood</em>” are used interchangeably.</p>
<p>More generally, the terms “<em>cross-entropy</em>” and “<em>negative log-likelihood</em>” are used interchangeably in the context of loss functions for classification models.</p>
<blockquote><p>The negative log-likelihood for logistic regression is given by […] This is also called the cross-entropy error function.</p></blockquote>
<p>— Page 246, <a href="https://amzn.to/2xKSTCP">Machine Learning: A Probabilistic Perspective</a>, 2012.</p>
<p>Therefore, calculating log loss will give the same quantity as 
calculating the cross-entropy for Bernoulli probability distribution. We
 can confirm this by calculating the log loss using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">log_loss() function</a> from the scikit-learn API.</p>
<p>Calculating the average log loss on the same set of actual and 
predicted probabilities from the previous section should give the same 
result as calculating the average cross-entropy.</p>
<p>The complete example is listed below.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de21889737472" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate log loss for classification problem with scikit-learn
from sklearn.metrics import log_loss
from numpy import asarray
# define classification data
p = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
q = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]
# define data as expected, e.g. probability for each event {0, 1}
y_true = asarray([[1-v, v] for v in p])
y_pred = asarray([[1-v, v] for v in q])
# calculate the average log loss
ll = log_loss(y_true, y_pred)
print('Average Log Loss: %.3f' % ll)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1de21889737472-12">12</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-1"><span class="crayon-p"># calculate log loss for classification problem with scikit-learn</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-2"><span class="crayon-e">from </span><span class="crayon-v">sklearn</span><span class="crayon-sy">.</span><span class="crayon-e">metrics </span><span class="crayon-e">import </span><span class="crayon-e">log_loss</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-3"><span class="crayon-e">from </span><span class="crayon-e">numpy </span><span class="crayon-e">import </span><span class="crayon-i">asarray</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-4"><span class="crayon-p"># define classification data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-5"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-6"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.9</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.6</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.8</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.4</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.1</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.3</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-7"><span class="crayon-p"># define data as expected, e.g. probability for each event {0, 1}</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-8"><span class="crayon-v">y_true</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-v">v</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">v</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">v</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-9"><span class="crayon-v">y_pred</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">asarray</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-sy">[</span><span class="crayon-cn">1</span><span class="crayon-o">-</span><span class="crayon-v">v</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">v</span><span class="crayon-sy">]</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">v</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-10"><span class="crayon-p"># calculate the average log loss</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-11"><span class="crayon-v">ll</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">log_loss</span><span class="crayon-sy">(</span><span class="crayon-v">y_true</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">y_pred</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1de21889737472-12"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'Average Log Loss: %.3f'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ll</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Running the example gives the expected result of 0.247 log 
loss, which matches 0.247 nats when calculated using the average 
cross-entropy.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1de23201218500" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; z-index: 4;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">Average Log Loss: 0.247</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1de23201218500-1">1</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1de23201218500-1">Average Log Loss: 0.247</div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>This does not mean that log loss calculates cross-entropy or cross-entropy calculates log loss.</p>
<p>Instead, they are different quantities, arrived at from different 
fields of study, that under the conditions of calculating a loss 
function for a classification task, result in an equivalent calculation 
and result. Specifically, a cross-entropy loss function is equivalent to
 a maximum likelihood function under a <a href="https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/">Bernoulli or Multinoulli probability distribution</a>.</p>
<p>This demonstrates a connection between the study of <a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/">maximum likelihood estimation</a> and information theory for discrete probability distributions.</p>
<p>It is not limited to discrete probability distributions, and this 
fact is surprising to many practitioners that hear it for the first 
time.</p>
<p>Specifically, a <a href="https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/">linear regression optimized under the maximum likelihood estimation framework</a>
 assumes a Gaussian continuous probability distribution for the target 
variable and involves minimizing the mean squared error function. This 
is equivalent to the cross-entropy for a random variable with a Gaussian
 probability distribution.</p>
<blockquote><p>Any loss consisting of a negative log-likelihood is a 
cross-entropy between the empirical distribution defined by the training
 set and the probability distribution defined by model. For example, 
mean squared error is the cross-entropy between the empirical 
distribution and a Gaussian model.</p></blockquote>
<p>— Page 132, <a href="https://amzn.to/2lnc3vL">Deep Learning</a>, 2016.</p>
<p>This is a little mind blowing, and comes from the field of <a href="https://en.wikipedia.org/wiki/Differential_entropy">differential entropy</a> for continuous random variables.</p>
<p>It means that if you calculate the mean squared error between two 
Gaussian random variables that cover the same events (have the same mean
 and standard deviation), then you are calculating the cross-entropy 
between the variables.</p>
<p>It also means that if you are using mean squared error loss to 
optimize your neural network model for a regression problem, you are in 
effect using a cross entropy loss.</p>
<h2>Further Reading</h2>
<p>This section provides more resources on the topic if you are looking to go deeper.</p>
<h3>Tutorials</h3>
<ul>
<li><a href="https://machinelearningmastery.com/what-is-information-entropy/">A Gentle Introduction to Information Entropy</a></li>
<li><a href="https://machinelearningmastery.com/divergence-between-probability-distributions/">How to Calculate the KL Divergence for Machine Learning</a></li>
<li><a href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/">A Gentle Introduction to Logistic Regression With Maximum Likelihood Estimation</a></li>
<li><a href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/">How to Choose Loss Functions When Training Deep Learning Neural Networks</a></li>
<li><a href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/">Loss and Loss Functions for Training Deep Learning Neural Networks</a></li>
</ul>
<h3>Books</h3>
<ul>
<li><a href="https://amzn.to/2xKSTCP">Machine Learning: A Probabilistic Perspective</a>, 2012.</li>
<li><a href="https://amzn.to/2JwHE7I">Pattern Recognition and Machine Learning</a>, 2006.</li>
<li><a href="https://amzn.to/2lnc3vL">Deep Learning</a>, 2016.</li>
</ul>
<h3>API</h3>
<ul>
<li><a href="https://keras.io/losses/">Usage of loss functions, Keras API</a>.</li>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">sklearn.metrics.log_loss API</a>.</li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence, Wikipedia</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy, Wikipedia</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Joint_entropy">Joint Entropy, Wikipedia</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">Loss functions for classification, Wikipedia.</a></li>
<li><a href="https://en.wikipedia.org/wiki/Differential_entropy">Differential entropy, Wikipedia</a>.</li>
</ul>
<h2>Summary</h2>
<p>In this tutorial, you discovered cross-entropy for machine learning.</p>
<p>Specifically, you learned:</p>
<ul>
<li>How to calculate cross-entropy from scratch and using standard machine learning libraries.</li>
<li>Cross-entropy can be used as a loss function when optimizing 
classification models like logistic regression and artificial neural 
networks.</li>
<li>Cross-entropy is different from KL divergence but can be calculated 
using KL divergence, and is different from log loss but calculates the 
same quantity when used as a loss function.</li>
</ul>
<p>Do you have any questions?<br>
Ask your questions in the comments below and I will do my best to answer.</p>
<div class="widget_text awac-wrapper" id="custom_html-63"><div class="widget_text awac widget custom_html-63"><div class="textwidget custom-html-widget"><div style="text-align: center;">
<div class="woo-sc-hr"></div>
<h2>Get a Handle on Probability for Machine Learning!</h2>
<a href="https://machinelearningmastery.com/probability-for-machine-learning/" rel="nofollow"><img style="border: 0;" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Cover-220-1.webp" alt="Probability for Machine Learning" width="220" height="311" align="left"></a>

<p></p><h4>Develop Your Understanding of Probability</h4>
...with just a few lines of python code<p></p>

<p>Discover how in my new Ebook:<br>
	<a href="https://machinelearningmastery.com/probability-for-machine-learning/" rel="nofollow">Probability for Machine Learning</a></p>

<p>It provides <strong>self-study tutorials</strong> and <strong>end-to-end projects</strong> on:<br>
	<em>Bayes Theorem</em>, <em>Bayesian Optimization</em>, <em>Distributions</em>, <em>Maximum Likelihood</em>, <em>Cross-Entropy</em>, <em>Calibrating Models</em><br>
	and much more...</p>

<p></p><h4>Finally Harness Uncertainty in Your Projects</h4>
Skip the Academics. Just Results.<p></p>

<a href="https://machinelearningmastery.com/probability-for-machine-learning/" class="woo-sc-button  red"><span class="woo-">See What's Inside</span></a>
<div class="woo-sc-hr"></div>
</div></div></div></div><div class="simplesocialbuttons simplesocial-simple-icons simplesocialbuttons_inline simplesocialbuttons-align-left post-8877 post  simplesocialbuttons-inline-no-animation">
<button class="ssb_tweet-icon" data-href="https://twitter.com/share?text=A+Gentle+Introduction+to+Cross-Entropy+for+Machine+Learning&amp;url=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="nofollow" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 72 72"><path fill="none" d="M0 0h72v72H0z"></path><path class="icon" fill="#fff" d="M68.812 15.14c-2.348 1.04-4.87 1.744-7.52 2.06 2.704-1.62 4.78-4.186 5.757-7.243-2.53 1.5-5.33 2.592-8.314 3.176C56.35 10.59 52.948 9 49.182 9c-7.23 0-13.092 5.86-13.092 13.093 0 1.026.118 2.02.338 2.98C25.543 24.527 15.9 19.318 9.44 11.396c-1.125 1.936-1.77 4.184-1.77 6.58 0 4.543 2.312 8.552 5.824 10.9-2.146-.07-4.165-.658-5.93-1.64-.002.056-.002.11-.002.163 0 6.345 4.513 11.638 10.504 12.84-1.1.298-2.256.457-3.45.457-.845 0-1.666-.078-2.464-.23 1.667 5.2 6.5 8.985 12.23 9.09-4.482 3.51-10.13 5.605-16.26 5.605-1.055 0-2.096-.06-3.122-.184 5.794 3.717 12.676 5.882 20.067 5.882 24.083 0 37.25-19.95 37.25-37.25 0-.565-.013-1.133-.038-1.693 2.558-1.847 4.778-4.15 6.532-6.774z"></path></svg></span><i class="simplesocialtxt">Tweet </i></button>
		<button class="ssb_fbshare-icon" target="_blank" data-href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" class="_1pbq" color="#ffffff"><path fill="#ffffff" fill-rule="evenodd" class="icon" d="M8 14H3.667C2.733 13.9 2 13.167 2 12.233V3.667A1.65 1.65 0 0 1 3.667 2h8.666A1.65 1.65 0 0 1 14 3.667v8.566c0 .934-.733 1.667-1.667 1.767H10v-3.967h1.3l.7-2.066h-2V6.933c0-.466.167-.9.867-.9H12v-1.8c.033 0-.933-.266-1.533-.266-1.267 0-2.434.7-2.467 2.133v1.867H6v2.066h2V14z"></path></svg></span>
						<span class="simplesocialtxt">Share </span> </button>
<button class="ssb_linkedin-icon" data-href="https://www.linkedin.com/cws/share?url=https://machinelearningmastery.com/cross-entropy-for-machine-learning/" onclick="if (!window.__cfRLUnblockHandlers) return false; javascript:window.open(this.dataset.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600');return false;">
						<span class="icon"> <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="15px" height="14.1px" viewBox="-301.4 387.5 15 14.1" enable-background="new -301.4 387.5 15 14.1" xml:space="preserve"> <g id="XMLID_398_"> <path id="XMLID_399_" fill="#FFFFFF" d="M-296.2,401.6c0-3.2,0-6.3,0-9.5h0.1c1,0,2,0,2.9,0c0.1,0,0.1,0,0.1,0.1c0,0.4,0,0.8,0,1.2 c0.1-0.1,0.2-0.3,0.3-0.4c0.5-0.7,1.2-1,2.1-1.1c0.8-0.1,1.5,0,2.2,0.3c0.7,0.4,1.2,0.8,1.5,1.4c0.4,0.8,0.6,1.7,0.6,2.5 c0,1.8,0,3.6,0,5.4v0.1c-1.1,0-2.1,0-3.2,0c0-0.1,0-0.1,0-0.2c0-1.6,0-3.2,0-4.8c0-0.4,0-0.8-0.2-1.2c-0.2-0.7-0.8-1-1.6-1 c-0.8,0.1-1.3,0.5-1.6,1.2c-0.1,0.2-0.1,0.5-0.1,0.8c0,1.7,0,3.4,0,5.1c0,0.2,0,0.2-0.2,0.2c-1,0-1.9,0-2.9,0 C-296.1,401.6-296.2,401.6-296.2,401.6z"></path> <path id="XMLID_400_" fill="#FFFFFF" d="M-298,401.6L-298,401.6c-1.1,0-2.1,0-3,0c-0.1,0-0.1,0-0.1-0.1c0-3.1,0-6.1,0-9.2 c0-0.1,0-0.1,0.1-0.1c1,0,2,0,2.9,0h0.1C-298,395.3-298,398.5-298,401.6z"></path> <path id="XMLID_401_" fill="#FFFFFF" d="M-299.6,390.9c-0.7-0.1-1.2-0.3-1.6-0.8c-0.5-0.8-0.2-2.1,1-2.4c0.6-0.2,1.2-0.1,1.8,0.2 c0.5,0.4,0.7,0.9,0.6,1.5c-0.1,0.7-0.5,1.1-1.1,1.3C-299.1,390.8-299.4,390.8-299.6,390.9L-299.6,390.9z"></path> </g> </svg> </span>
						<span class="simplesocialtxt">Share</span> </button>
</div>
	</section><!-- /.entry -->
	<div class="fix"></div>
<aside id="post-author">
	<div class="profile-image"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_003.png 2x" class="avatar avatar-80 photo" loading="lazy" width="80" height="80"></div>
	<div class="profile-content">
		<h4>About Jason Brownlee</h4>
		Jason Brownlee, PhD is a machine learning specialist who teaches 
developers how to get results with modern machine learning methods via 
hands-on tutorials.				<div class="profile-link">
			<a href="https://machinelearningmastery.com/author/jasonb/">
				View all posts by Jason Brownlee <span class="meta-nav">→</span>			</a>
		</div><!--#profile-link-->
			</div>
	<div class="fix"></div>
</aside>
<div class="post-utility"></div>
</article><!-- /.post -->
	        <div class="post-entries">
	            <div class="nav-prev fl"><a href="https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/" rel="prev"><i class="fa fa-angle-left"></i> Naive Bayes Classifier From Scratch in Python</a></div>
	            <div class="nav-next fr"><a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/" rel="next">A Gentle Introduction to Maximum Likelihood Estimation for Machine Learning <i class="fa fa-angle-right"></i></a></div>
	            <div class="fix"></div>
	        </div>

		<div id="comments">		 	<h3 id="comments-title">49 Responses to <em>A Gentle Introduction to Cross-Entropy for Machine Learning</em></h3>
		 	<ol class="commentlist">
				
		<li id="comment-506807" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-506807" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/c7a0f1b5244053e52dc9481c149d8bf7.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/c7a0f1b5244053e52dc9481c149d8bf7_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Markus</span>
	                <span class="date">October 22, 2019 at 6:57 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-506807" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hi</p>
<p>Thanks for this blog post.</p>
<p>What confuses me a bit is the fact that we interpret the labels 0 and
 1 in the example as the probability values for calculating the cross 
entropy between the target distribution and the predicted distribution!</p>
<p>What if the labels were 4 and 7 instead of 0 and 1?!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-506807" data-commentid="506807" data-postid="8877" data-belowelement="comment-506807" data-respondelement="respond" data-replyto="Reply to Markus" aria-label="Reply to Markus">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-506845" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-506845" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 22, 2019 at 1:44 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-506845" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks.</p>
<p>Good question. For binary classification we map the labels, whatever they are to 0 and 1.</p>
<p>I recommend reading about the Bernoulli distribution:<br>
<a href="https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/" rel="nofollow ugc">https://machinelearningmastery.com/discrete-probability-distributions-for-machine-learning/</a></p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-506845" data-commentid="506845" data-postid="8877" data-belowelement="comment-506845" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-506880" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-506880" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/50faaef7fccdb4bc8f34083410234550.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/50faaef7fccdb4bc8f34083410234550_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">sharpblade4</span>
	                <span class="date">October 22, 2019 at 5:35 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-506880" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hi Jason,<br>
A small fix suggestion: in the beginning of the article in section “What
 Is Cross-Entropy?” you’ve mentioned that “The result will be a positive
 number measured in bits and 0 if the two probability distributions are 
identical.”.<br>
However, the cross entropy for the same probability-distributions H(P,P)
 is the entropy for the probability-distribution H(P), opposed to KL 
divergence of the same probability-distribution which would indeed 
outcome zero.</p>
<p>Thanks,<br>
Ron U</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-506880" data-commentid="506880" data-postid="8877" data-belowelement="comment-506880" data-respondelement="respond" data-replyto="Reply to sharpblade4" aria-label="Reply to sharpblade4">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-506977" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-506977" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 23, 2019 at 6:34 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-506977" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks Ron!</p>
<p>I’ll schedule time to update the post and give an example of exactly what you’re referring to. E.g.:</p>
<p></p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f77c466213148" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># example of calculating cross entropy
from math import log2

# calculate cross entropy
def cross_entropy(p, q):
	return -sum([p[i]*log2(q[i]) for i in range(len(p))])

# define data
p = [0.10, 0.40, 0.50]
q = [0.80, 0.15, 0.05]
# calculate cross entropy H(P, P)
ce_pp = cross_entropy(p, p)
print('H(P, P): %.3f bits' % ce_pp)
# calculate cross entropy H(Q, Q)
ce_qq = cross_entropy(q, q)
print('H(Q, Q): %.3f bits' % ce_qq)</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-3">3</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-4">4</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-5">5</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-6">6</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-7">7</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-8">8</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-9">9</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-10">10</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-11">11</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-12">12</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-13">13</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-14">14</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-15">15</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f77c466213148-16">16</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-1"><span class="crayon-p"># example of calculating cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-2"><span class="crayon-e">from </span><span class="crayon-e">math </span><span class="crayon-e">import </span><span class="crayon-i">log2</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-3">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-4"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-5"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-6"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-sy">[</span><span class="crayon-v">p</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-o">*</span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">[</span><span class="crayon-v">i</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-i">i</span><span class="crayon-h"> </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">range</span><span class="crayon-sy">(</span><span class="crayon-e">len</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span><span class="crayon-sy">]</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-7">&nbsp;</div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-8"><span class="crayon-p"># define data</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-9"><span class="crayon-v">p</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.10</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.40</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.50</span><span class="crayon-sy">]</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-10"><span class="crayon-v">q</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-sy">[</span><span class="crayon-cn">0.80</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.15</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-sy">]</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-11"><span class="crayon-p"># calculate cross entropy H(P, P)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-12"><span class="crayon-v">ce_pp</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">p</span><span class="crayon-sy">)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-13"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(P, P): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_pp</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-14"><span class="crayon-p"># calculate cross entropy H(Q, Q)</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-15"><span class="crayon-v">ce_qq</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">q</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f77c466213148-16"><span class="crayon-e">print</span><span class="crayon-sy">(</span><span class="crayon-s">'H(Q, Q): %.3f bits'</span><span class="crayon-h"> </span><span class="crayon-o">%</span><span class="crayon-h"> </span><span class="crayon-v">ce_qq</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<p>H(P, P): 1.361 bits<br>
H(Q, Q): 0.884 bits</p>
<p>Update: I have updated the post to correctly discuss this case. </p>
<p>Thanks Again!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-506977" data-commentid="506977" data-postid="8877" data-belowelement="comment-506977" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-507547" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-507547" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/bbcac97b68f9e9cabd25a3f0f0bdbe49.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/bbcac97b68f9e9cabd25a3f0f0bdbe49_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Hugh Brown</span>
	                <span class="date">October 26, 2019 at 4:39 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507547" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Why not use <code>zip</code>?</p>
<p></p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f753029494646" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft"># calculate cross entropy
def cross_entropy(p, q):
	return -sum(pp*log2(qq) for pp, qq in zip(p, q))</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f753029494646-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f753029494646-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f753029494646-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f753029494646-1"><span class="crayon-p"># calculate cross entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f753029494646-2"><span class="crayon-e">def </span><span class="crayon-e">cross_entropy</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f753029494646-3"><span class="crayon-h">	</span><span class="crayon-st">return</span><span class="crayon-h"> </span><span class="crayon-o">-</span><span class="crayon-e">sum</span><span class="crayon-sy">(</span><span class="crayon-e ">pp*</span><span class="crayon-e">log2</span><span class="crayon-sy">(</span><span class="crayon-v">qq</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-st">for</span><span class="crayon-h"> </span><span class="crayon-v">pp</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-e">qq </span><span class="crayon-st">in</span><span class="crayon-h"> </span><span class="crayon-e">zip</span><span class="crayon-sy">(</span><span class="crayon-v">p</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">q</span><span class="crayon-sy">)</span><span class="crayon-sy">)</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507547" data-commentid="507547" data-postid="8877" data-belowelement="comment-507547" data-respondelement="respond" data-replyto="Reply to Hugh Brown" aria-label="Reply to Hugh Brown">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-507602" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-507602" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 27, 2019 at 5:43 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507602" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks for the tip Hugh, that is a much cleaner approach!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507602" data-commentid="507602" data-postid="8877" data-belowelement="comment-507602" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-507667" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-507667" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/4cacef8937b1a3db309c7ba456c6f66a.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/4cacef8937b1a3db309c7ba456c6f66a_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Anthony The Koala</span>
	                <span class="date">October 28, 2019 at 3:48 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507667" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Dear Dr Jason,<br>
In the last few lines under the subheading “How to Calculate 
Cross-Entropy”, you had the simple example with the following outputs:</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f6cc446602132" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P): 1.361 bits          #entropy
KL(P || Q): 1.927 bits #kl divergence
H(P, Q): 3.288 bits     #cross entropy = entropy + kl divergence</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-1"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">1.361</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-p">#entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-2"><span class="crayon-e">KL</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-h"> </span><span class="crayon-o">||</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">1.927</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h"> </span><span class="crayon-p">#kl divergence</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6cc446602132-3"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">3.288</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="crayon-p">#cross entropy = entropy + kl divergence</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<p>What is the interpretation of these figures in ‘plain English’ please.</p>
<p>For example if the above example produced the following result:</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f6d0989715222" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P): 0.361 bits          #entropy
KL(P || Q): 2.927 bits #kl divergence
H(P, Q): 3.288 bits     #cross entropy = entropy + kl divergence</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-1"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">0.361</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-p">#entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-2"><span class="crayon-e">KL</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-h"> </span><span class="crayon-o">||</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">2.927</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h"> </span><span class="crayon-p">#kl divergence</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d0989715222-3"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">3.288</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="crayon-p">#cross entropy = entropy + kl divergence</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p>Or if we had this:</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f6d1551995394" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P): 1.927 bits          #entropy
KL(P || Q): 0.361 bits #kl divergence
H(P, Q): 3.288 bits     #cross entropy = entropy + kl divergence</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-1"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">1.927</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="crayon-p">#entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-2"><span class="crayon-e">KL</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-h"> </span><span class="crayon-o">||</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">0.361</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h"> </span><span class="crayon-p">#kl divergence</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d1551995394-3"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">,</span><span class="crayon-h"> </span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-o">:</span><span class="crayon-h"> </span><span class="crayon-cn">3.288</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="crayon-p">#cross entropy = entropy + kl divergence</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<p>Here is another example of made up figures.</p>
		<div id="urvanov-syntax-highlighter-605b5c6d1f6d2244954313" class="urvanov-syntax-highlighter-syntax crayon-theme-classic urvanov-syntax-highlighter-font-monaco urvanov-syntax-highlighter-os-pc print-yes notranslate" data-settings=" minimize scroll-mouseover disable-anim" style="margin-top: 12px; margin-bottom: 12px; font-size: 12px !important; line-height: 15px !important; height: auto;">
		
			<div class="crayon-toolbar" data-settings=" mouseover overlay hide delay" style="font-size: 12px !important; height: 18px !important; line-height: 18px !important; margin-top: -19px; display: none; position: absolute; z-index: 2;"><span class="crayon-title"></span>
			<div class="crayon-tools" style="font-size: 12px !important;height: 18px !important; line-height: 18px !important;"><div class="crayon-button urvanov-syntax-highlighter-nums-button crayon-pressed" title="Toggle Line Numbers"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-plain-button" title="Toggle Plain Code"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-wrap-button" title="Toggle Line Wrap"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-expand-button" title="Expand Code" style="display: none;"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-copy-button" title="Copy"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div><div class="crayon-button urvanov-syntax-highlighter-popup-button" title="Open Code In New Window"><div class="urvanov-syntax-highlighter-button-icon" style="background-size: 48px 128px; background-image: url(&quot;https://machinelearningmastery.com/wp-content/plugins/urvanov-syntax-highlighter/css/images/toolbar/buttons@2x.png&quot;);"></div></div></div></div>
			<div class="crayon-info" style="min-height: 16.8px !important; line-height: 16.8px !important;"></div>
			<div class="urvanov-syntax-highlighter-plain-wrap"><textarea class="urvanov-syntax-highlighter-plain print-no" data-settings="dblclick" readonly="readonly" style="-moz-tab-size: 4; font-size: 12px !important; line-height: 15px !important; z-index: 0; opacity: 0; overflow: hidden;" wrap="soft">H(P) = 0.05 bits       #entropy
KL(P||Q) = 0.2 bits  #kl diverengence
H(P,Q) = 0.25 bits   #cross entropy = entropy + kl divergence.</textarea></div>
			<div class="urvanov-syntax-highlighter-main" style="position: relative; z-index: 1; overflow: hidden;">
				<table class="crayon-table" style="">
					<tbody><tr class="urvanov-syntax-highlighter-row">
				<td class="crayon-nums " data-settings="show">
					<div class="urvanov-syntax-highlighter-nums-content" style="font-size: 12px !important; line-height: 15px !important;"><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-1">1</div><div class="crayon-num crayon-striped-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-2">2</div><div class="crayon-num" data-line="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-3">3</div></div>
				</td>
						<td class="urvanov-syntax-highlighter-code"><div class="crayon-pre" style="font-size: 12px !important; line-height: 15px !important; -moz-tab-size:4; -o-tab-size:4; -webkit-tab-size:4; tab-size:4;"><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-1"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-cn">0.05</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="crayon-p">#entropy</span></div><div class="crayon-line crayon-striped-line" id="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-2"><span class="crayon-e">KL</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-o">||</span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-cn">0.2</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp;</span><span class="crayon-p">#kl diverengence</span></div><div class="crayon-line" id="urvanov-syntax-highlighter-605b5c6d1f6d2244954313-3"><span class="crayon-e">H</span><span class="crayon-sy">(</span><span class="crayon-v">P</span><span class="crayon-sy">,</span><span class="crayon-v">Q</span><span class="crayon-sy">)</span><span class="crayon-h"> </span><span class="crayon-o">=</span><span class="crayon-h"> </span><span class="crayon-cn">0.25</span><span class="crayon-h"> </span><span class="crayon-i">bits</span><span class="crayon-h">&nbsp;&nbsp; </span><span class="crayon-p">#cross entropy = entropy + kl divergence.</span></div></div></td>
					</tr>
				</tbody></table>
			</div>
		</div><p></p>
<p>Comparing the first output to the ‘made up figures’ does the lower 
the number of bits mean a better fit? A more predictable model?</p>
<p>Also:<br>
I understand that a bit is a base 2 number. Eg 1 = 1(base 10), 11 = 3 (base 10), 101 = 5 (base 10).<br>
The number of bits in a base 2 system is an integer. What does a fraction of bit mean?</p>
<p>Anthony of Sydney</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507667" data-commentid="507667" data-postid="8877" data-belowelement="comment-507667" data-respondelement="respond" data-replyto="Reply to Anthony The Koala" aria-label="Reply to Anthony The Koala">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-507692" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2 parent">

	      	<div id="li-comment-507692" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 28, 2019 at 6:10 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507692" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Typically we use cross-entropy to evaluate a model, e.g. true classes vs probability predictions.</p>
<p>In that case would compare the average cross-entropy calculated 
across all examples and a lower value would represent a better fit. 
Interpreting the specific figures is often not useful.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507692" data-commentid="507692" data-postid="8877" data-belowelement="comment-507692" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-507694" class="comment even depth-3 parent">

	      	<div id="li-comment-507694" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/4cacef8937b1a3db309c7ba456c6f66a.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/4cacef8937b1a3db309c7ba456c6f66a_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Anthony The Koala</span>
	                <span class="date">October 28, 2019 at 6:18 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507694" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Dear Dr Jason,<br>
Thank you for response.<br>
The other question please:<br>
How can you have a fraction of a bit. For example entropy = 3.2285 bits. What is 0.2285 bits.<br>
Thank you,<br>
Anthony of Sydney</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507694" data-commentid="507694" data-postid="8877" data-belowelement="comment-507694" data-respondelement="respond" data-replyto="Reply to Anthony The Koala" aria-label="Reply to Anthony The Koala">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-507704" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-4">

	      	<div id="li-comment-507704" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 28, 2019 at 7:00 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507704" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Recall, it is an average over a distribution with many events.</p>
<p>Think of it more of a measure and less like the crisp bits in a computer.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507704" data-commentid="507704" data-postid="8877" data-belowelement="comment-507704" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-507747" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-507747" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/c5dab5a905dfd27190d4e49b2d0e0657_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/c5dab5a905dfd27190d4e49b2d0e0657.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Farhan</span>
	                <span class="date">October 28, 2019 at 6:32 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507747" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Great Article, Hope to see more more content on machine learning and AI.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507747" data-commentid="507747" data-postid="8877" data-belowelement="comment-507747" data-respondelement="respond" data-replyto="Reply to Farhan" aria-label="Reply to Farhan">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-507793" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-507793" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 29, 2019 at 5:21 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-507793" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-507793" data-commentid="507793" data-postid="8877" data-belowelement="comment-507793" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-508047" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-508047" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/8936e20f8fc748419f3058c22c58df53_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/8936e20f8fc748419f3058c22c58df53.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Confused</span>
	                <span class="date">October 31, 2019 at 1:26 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-508047" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hi,</p>
<p>could you provide an example of this sentence “The entropy for a 
distribution of all 0 or all 1 values or mixtures of these values will 
equal 0.0.”? </p>
<p>Does this mean a distribution with a mixture of these values, eg. the
 distribution with P(X=1) = 0.4 and P(X=0) = 0.6 has entropy zero? But 
this should not be the case because 0.4 * log(0.4) + 0.6 * log(0.6) is 
not zero. </p>
<p>Thank you!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-508047" data-commentid="508047" data-postid="8877" data-belowelement="comment-508047" data-respondelement="respond" data-replyto="Reply to Confused" aria-label="Reply to Confused">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-508055" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-508055" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">October 31, 2019 at 1:46 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-508055" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Good question.</p>
<p>Sorry that is confusing.</p>
<p>I mean that the probability distribution for a class label will always be zero.</p>
<p>I have updated the tutorial to be clearer and given a worked example.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-508055" data-commentid="508055" data-postid="8877" data-belowelement="comment-508055" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-510413" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-510413" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/6b9712086803b79e38282e637da41981_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/6b9712086803b79e38282e637da41981.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Xin</span>
	                <span class="date">November 13, 2019 at 9:35 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-510413" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hi Jason! This is the best article I’ve ever seen on cross 
entropy and KL-divergence! Finally I can understand them 😉 Thank you so
 much for the comprehensive article.</p>
<p>I have one small question: in the secion “Intuition for Cross-Entropy
 on Predicted Probabilities”, in the first code block to plot the 
visualization, the code is as follows:</p>
<p># define the target distribution for two events<br>
target = [0.0, 0.1]<br>
# define probabilities for the first event<br>
probs = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]<br>
# create probability distributions for the two events<br>
dists = [[p, 1.0 – p] for p in probs]<br>
# calculate cross-entropy for each distribution<br>
ents = [cross_entropy(target, d) for d in dists]</p>
<p>I do not quite understand why the target probability for the two events are [0.0, 0.1]? Could you explain a bit more? Thank you!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-510413" data-commentid="510413" data-postid="8877" data-belowelement="comment-510413" data-respondelement="respond" data-replyto="Reply to Xin" aria-label="Reply to Xin">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-510442" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-510442" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">November 13, 2019 at 1:45 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-510442" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks! I worked really hard on it and I’m so happy that it’s appreciated 🙂</p>
<p>Yes, looks like a typo. I’ll fix it ASAP. It should be [0,1]. </p>
<p>Update: I have updated the code and re-generated the plots. Thanks again!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-510442" data-commentid="510442" data-postid="8877" data-belowelement="comment-510442" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-515058" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-515058" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/8fa687c58de58b5f27a5077c6f9be7cb_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/8fa687c58de58b5f27a5077c6f9be7cb.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Scott</span>
	                <span class="date">December 14, 2019 at 8:36 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-515058" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Jason, I so appreciate all your various posts on ML topics.  If I
 may add one comment regarding what I’ve found helpful in the past: </p>
<p>One point that I didn’t see really emphasized here that I’ve seen in other treatments (e.g., <a href="https://tdhopper.com/blog/cross-entropy-and-kl-divergence/" rel="nofollow ugc">https://tdhopper.com/blog/cross-entropy-and-kl-divergence/</a>) is that cross-entropy and KL difference “differ by a constant”, i.e. in your expression</p>
<p>H(P, Q) = H(P) + KL(P || Q),</p>
<p>the H(P) is constant with respect to Q.  In most ML tasks, P is 
usually fixed as the “true” distribution” and Q is the distribution we 
are iteratively trying to refine until it matches P. </p>
<p>“In many of these situations, 𝑝 is treated as the ‘true’ 
distribution, and 𝑞 as the model that we’re trying to optimize…. 
Because 𝑝 is fixed, 𝐻(𝑝) doesn’t change with the parameters of the 
model, and can be disregarded in the loss function.” (<a href="https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne/265989" rel="nofollow ugc">https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne/265989</a>)   </p>
<p>You do get to this when you say “As such, minimizing the KL 
divergence and the cross entropy for a classification task are 
identical.”  </p>
<p>And yet for me at least, knowing that the two “differ by a constant” 
makes it intuitively obvious why minimizing one is the same as 
minimizing the other, even if they’re actually intended to measure 
different things. </p>
<p>…Thus I think this “differ by a constant” is another reason that 
people get mixed up about cross-entropy vs KL divergence, and why guides
 like yours are so helpful!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-515058" data-commentid="515058" data-postid="8877" data-belowelement="comment-515058" data-respondelement="respond" data-replyto="Reply to Scott" aria-label="Reply to Scott">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-515180" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-515180" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">December 15, 2019 at 6:01 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-515180" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks for your note Scott.</p>
<p>Yes, H(P) is the entropy of the distribution. This becomes 0 when 
class labels are 0 and 1. I outline this at the end of the post when we 
talk about class labels.</p>
<p>A constant of 0 in that case means using KL divergence and cross entropy result in the same numbers, e.g. the kl divergence.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-515180" data-commentid="515180" data-postid="8877" data-belowelement="comment-515180" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-521227" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-521227" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/e8010eff1b26f76d53ea1861a16a4000_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/e8010eff1b26f76d53ea1861a16a4000.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Arij</span>
	                <span class="date">February 11, 2020 at 6:52 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-521227" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>thanks for a grate article!<br>
but what confused me that in your article you have mentioned that<br>
”<br>
The result will be a positive number measured in bits and will be equal 
to the entropy of the distribution if the two probability distributions 
are identical.”</p>
<p>then again mentioned</p>
<p>“If two probability distributions are the same, then the cross-entropy between them will be the entropy of the distribution.”</p>
<p>then</p>
<p>“This means that the cross entropy of two distributions (real and 
predicted) that have the same probability distribution for a class 
label, will also always be 0.0.”</p>
<p>“Therefore, a cross-entropy of 0.0 when training a model indicates 
that the predicted class probabilities are identical to the 
probabilities in the training dataset, e.g. zero loss.”</p>
<p>Am I missing something?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-521227" data-commentid="521227" data-postid="8877" data-belowelement="comment-521227" data-respondelement="respond" data-replyto="Reply to Arij" aria-label="Reply to Arij">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-521248" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-521248" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">February 11, 2020 at 1:42 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-521248" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>The statements are correct.</p>
<p>The cross-entropy will be the entropy between the distributions if the distributions are identical.</p>
<p>In this case, if we are working with class labels like 0 and 1, then the entropy for two identical distributions will be zero.</p>
<p>We demonstrate this with a worked example in the above tutorial.</p>
<p>Does that help?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-521248" data-commentid="521248" data-postid="8877" data-belowelement="comment-521248" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-521264" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-521264" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/e8010eff1b26f76d53ea1861a16a4000_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/e8010eff1b26f76d53ea1861a16a4000.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Arij</span>
	                <span class="date">February 11, 2020 at 5:36 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-521264" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>it was not about examples, they were understandable, thanks.<br>
Just I could not imagine and understand them numerically.<br>
Reading them again I understand that when the values of any distribution
 are only one or zero then entropy, cross-entropy, KL all will be zero.</p>
<p>they will have values just in case they have values between 0 and 1 also</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-521264" data-commentid="521264" data-postid="8877" data-belowelement="comment-521264" data-respondelement="respond" data-replyto="Reply to Arij" aria-label="Reply to Arij">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-521324" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-521324" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">February 12, 2020 at 5:43 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-521324" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Exactly!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-521324" data-commentid="521324" data-postid="8877" data-belowelement="comment-521324" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-530157" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-530157" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/b49c66bb9720ccd75996539b570ba20d_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/b49c66bb9720ccd75996539b570ba20d.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Grzegorz Kępisty</span>
	                <span class="date">April 17, 2020 at 4:23 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-530157" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Good morning Jason!</p>
<p>Thank you for great post!</p>
<p>Question on KL Divergence: In its definition we have log2(p[i]/q[i]) 
which suggests a possibility of zero division error. Is it a probable 
issue in real applications? Or for some reason it does not occur?<br>
Regards!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-530157" data-commentid="530157" data-postid="8877" data-belowelement="comment-530157" data-respondelement="respond" data-replyto="Reply to Grzegorz Kępisty" aria-label="Reply to Grzegorz Kępisty">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-530258" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-530258" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">April 18, 2020 at 5:41 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-530258" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Good question, no problem as probabilities are always greater than zero, so log never blows up.</p>
<p>More on kl divergence here too:<br>
<a href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="nofollow ugc">https://machinelearningmastery.com/divergence-between-probability-distributions/</a></p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-530258" data-commentid="530258" data-postid="8877" data-belowelement="comment-530258" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-531858" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-531858" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75178a03b9a6615a9bddf61cf2efa5.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75178a03b9a6615a9bddf61cf2efa5_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://n/A" rel="external nofollow ugc" class="url">Allan</a></span>
	                <span class="date">April 27, 2020 at 2:25 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-531858" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Is it possible to use KL divergence as a classification criterion?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-531858" data-commentid="531858" data-postid="8877" data-belowelement="comment-531858" data-respondelement="respond" data-replyto="Reply to Allan" aria-label="Reply to Allan">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-531919" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2 parent">

	      	<div id="li-comment-531919" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">April 27, 2020 at 5:37 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-531919" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Probably, it would be the same as log loss and cross entropy when using class labels instead of probabilities.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-531919" data-commentid="531919" data-postid="8877" data-belowelement="comment-531919" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-531972" class="comment even depth-3 parent">

	      	<div id="li-comment-531972" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75178a03b9a6615a9bddf61cf2efa5.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75178a03b9a6615a9bddf61cf2efa5_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://n/A" rel="external nofollow ugc" class="url">Allan</a></span>
	                <span class="date">April 27, 2020 at 12:46 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-531972" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks for your reply. So let say the final calculation result is “Average Log Loss”, what does this value implies meaning?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-531972" data-commentid="531972" data-postid="8877" data-belowelement="comment-531972" data-respondelement="respond" data-replyto="Reply to Allan" aria-label="Reply to Allan">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-532078" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-4">

	      	<div id="li-comment-532078" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">April 28, 2020 at 6:39 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-532078" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Average difference between the probability distributions of expected and predicted values in bits.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-532078" data-commentid="532078" data-postid="8877" data-belowelement="comment-532078" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-532259" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-532259" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ee16e306d34cfc9ee929185bb787bba_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ee16e306d34cfc9ee929185bb787bba.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Gledson</span>
	                <span class="date">April 29, 2020 at 6:21 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-532259" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hello Jason, Congratulations on the explanation. How are you? I 
have a doubt. If I have log(0), I get -Inf on my crossentropy. Should I 
replace -Inf with some value? If so, what value?</p>
<p>Best,<br>
Gledson.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-532259" data-commentid="532259" data-postid="8877" data-belowelement="comment-532259" data-respondelement="respond" data-replyto="Reply to Gledson" aria-label="Reply to Gledson">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-532283" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-532283" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">April 29, 2020 at 6:36 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-532283" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>You cannot log a zero. It is a good idea to always add a tiny value to anything to log, e.g. log(value + 1e-8)</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-532283" data-commentid="532283" data-postid="8877" data-belowelement="comment-532283" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-534467" class="comment even thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-534467" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2562f3a51c094ae3282baa074cb9910b.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2562f3a51c094ae3282baa074cb9910b_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Zeinab</span>
	                <span class="date">May 13, 2020 at 3:47 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534467" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hi Jason,</p>
<p>Thanks for all your great post, I’ve read some of them.</p>
<p>I’m working on traffic classification and I’ve converted my data to string of bits, I want to use cross-entropy on bytes. </p>
<p>Assume below lists:</p>
<p>p = [1, 0, 1, 1, 0, 0, 1, 0]<br>
q = [1, 1, 1, 0, 1, 0, 0, 1]</p>
<p>When I use <code>-sum([p[i] * log2(q[i]) for i in range(len(p))])</code>, I encounter this error :ValueError: math domain error. S</p>
<p>Would you please tell me what I’m doing wrong here and how can I implement cross-entropy on a list of bits?</p>
<p>Bests,</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534467" data-commentid="534467" data-postid="8877" data-belowelement="comment-534467" data-respondelement="respond" data-replyto="Reply to Zeinab" aria-label="Reply to Zeinab">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-534513" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2 parent">

	      	<div id="li-comment-534513" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">May 13, 2020 at 6:43 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534513" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Can’t calculate log of 0.0. Try adding a tiny value to the equation, e.g. 1e-8 or 1e-15</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534513" data-commentid="534513" data-postid="8877" data-belowelement="comment-534513" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-534579" class="comment even depth-3">

	      	<div id="li-comment-534579" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2562f3a51c094ae3282baa074cb9910b.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2562f3a51c094ae3282baa074cb9910b_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Zeinab</span>
	                <span class="date">May 13, 2020 at 11:03 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534579" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>I’ve converted the traffic to string of bits, it’s not just some random numbers that I can add any value.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534579" data-commentid="534579" data-postid="8877" data-belowelement="comment-534579" data-respondelement="respond" data-replyto="Reply to Zeinab" aria-label="Reply to Zeinab">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-534565" class="comment odd alt thread-even depth-1 parent">

	      	<div id="li-comment-534565" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ad4edf39fa279d8eac13f72f151cba6.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ad4edf39fa279d8eac13f72f151cba6_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">TuanVu</span>
	                <span class="date">May 13, 2020 at 7:58 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534565" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Why we use log function for cross entropy?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534565" data-commentid="534565" data-postid="8877" data-belowelement="comment-534565" data-respondelement="respond" data-replyto="Reply to TuanVu" aria-label="Reply to TuanVu">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-534621" class="comment byuser comment-author-jasonb bypostauthor even depth-2">

	      	<div id="li-comment-534621" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">May 14, 2020 at 5:46 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534621" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Good question, perhaps start here:<br>
<a href="https://machinelearningmastery.com/what-is-information-entropy/" rel="nofollow ugc">https://machinelearningmastery.com/what-is-information-entropy/</a></p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534621" data-commentid="534621" data-postid="8877" data-belowelement="comment-534621" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-534665" class="comment odd alt thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-534665" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ad4edf39fa279d8eac13f72f151cba6.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1ad4edf39fa279d8eac13f72f151cba6_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">TuanVu</span>
	                <span class="date">May 14, 2020 at 9:22 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534665" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>thanks you !!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534665" data-commentid="534665" data-postid="8877" data-belowelement="comment-534665" data-respondelement="respond" data-replyto="Reply to TuanVu" aria-label="Reply to TuanVu">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-534691" class="comment byuser comment-author-jasonb bypostauthor even depth-2">

	      	<div id="li-comment-534691" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">May 14, 2020 at 1:25 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-534691" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>You’re welcome.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-534691" data-commentid="534691" data-postid="8877" data-belowelement="comment-534691" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-537065" class="comment odd alt thread-even depth-1 parent">

	      	<div id="li-comment-537065" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/93d0d1dae6b72d6fa78aca152151cd2f.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/93d0d1dae6b72d6fa78aca152151cd2f_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Zahir</span>
	                <span class="date">May 29, 2020 at 3:36 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-537065" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Hello Jason,<br>
Thank you so much for all your great posts.<br>
I have a quesion, if we have conditional entropy H(y|x)=-sum P(x,y) log(P(y|x)<br>
could we say that it is equal to cross-entropy H( x,y) = – sum y log y^?<br>
And if that correct where we could say that? i.e., under what assumptions. </p>
<p>Thank you in advance.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-537065" data-commentid="537065" data-postid="8877" data-belowelement="comment-537065" data-respondelement="respond" data-replyto="Reply to Zahir" aria-label="Reply to Zahir">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-537147" class="comment byuser comment-author-jasonb bypostauthor even depth-2 parent">

	      	<div id="li-comment-537147" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">May 30, 2020 at 5:52 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-537147" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>I think you’re asking me if the conditional entropy is the same 
as the cross entropy. I don’t think it is off the cuff, but perhaps 
confirm with a good textbook.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-537147" data-commentid="537147" data-postid="8877" data-belowelement="comment-537147" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-537189" class="comment odd alt depth-3 parent">

	      	<div id="li-comment-537189" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/93d0d1dae6b72d6fa78aca152151cd2f.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/93d0d1dae6b72d6fa78aca152151cd2f_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Zahir</span>
	                <span class="date">May 30, 2020 at 10:23 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-537189" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thank you so much  for your replay,<br>
I found it in “Privacy-Preserving Adversarial Networks” paper, the 
authors get a conditional entropy as a cost function, but when they 
implement the article, they use cross-entropy. But they don’t say why?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-537189" data-commentid="537189" data-postid="8877" data-belowelement="comment-537189" data-respondelement="respond" data-replyto="Reply to Zahir" aria-label="Reply to Zahir">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-537318" class="comment byuser comment-author-jasonb bypostauthor even depth-4">

	      	<div id="li-comment-537318" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">May 31, 2020 at 6:17 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-537318" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Fascinating.</p>
<p>Perhaps email the authors directly.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-537318" data-commentid="537318" data-postid="8877" data-belowelement="comment-537318" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-572189" class="comment odd alt thread-odd thread-alt depth-1 parent">

	      	<div id="li-comment-572189" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2d2872ab421e0866e215c5bb5893734f.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/2d2872ab421e0866e215c5bb5893734f_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="https://arxiv.org/abs/1611.09913" rel="external nofollow ugc" class="url">BARURI SAI AVINASH</a></span>
	                <span class="date">November 1, 2020 at 4:22 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-572189" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>How can be Number of bits per charecter in text generation  is equal to loss ???</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-572189" data-commentid="572189" data-postid="8877" data-belowelement="comment-572189" data-respondelement="respond" data-replyto="Reply to BARURI SAI AVINASH" aria-label="Reply to BARURI SAI AVINASH">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-572327" class="comment byuser comment-author-jasonb bypostauthor even depth-2">

	      	<div id="li-comment-572327" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">November 2, 2020 at 6:38 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-572327" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Perhaps try re-reading the above tutorial that lays it all out.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-572327" data-commentid="572327" data-postid="8877" data-belowelement="comment-572327" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-587486" class="comment odd alt thread-even depth-1 parent">

	      	<div id="li-comment-587486" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/395aa700cfa36ed8677eed45cfca5a47_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/395aa700cfa36ed8677eed45cfca5a47.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Eric Orr</span>
	                <span class="date">December 22, 2020 at 11:05 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-587486" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>This is excellent Introduction to Cross-Entropy.  It seems that 
one of the following sentences may have a typo in the stated notion of 
“surprise”.  My first impression is that the second sentence should have
 said “are less surprising”.  Is that true?</p>
<p>“Low probability events are more surprising therefore have a larger 
amount of information. Whereas probability distributions where the 
events are equally likely are more surprising and have larger entropy.”</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-587486" data-commentid="587486" data-postid="8877" data-belowelement="comment-587486" data-respondelement="respond" data-replyto="Reply to Eric Orr" aria-label="Reply to Eric Orr">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-587549" class="comment byuser comment-author-jasonb bypostauthor even depth-2">

	      	<div id="li-comment-587549" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">December 22, 2020 at 1:34 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-587549" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks.</p>
<p>Yes it could be clearer. Information is about events, entropy is 
about distributions, cross-entropy is about comparing distributions.</p>
<p>Surprise means something different when talking about information/events as compared to entropy/distributions.</p>
<p>I mixed the discussion of the two at the start of the tutorial. I have updated the text to be clearer. </p>
<p>Also see this:<br>
<a href="https://machinelearningmastery.com/what-is-information-entropy/" rel="nofollow ugc">https://machinelearningmastery.com/what-is-information-entropy/</a></p>
<p>Does that help?</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-587549" data-commentid="587549" data-postid="8877" data-belowelement="comment-587549" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-587506" class="comment odd alt thread-odd thread-alt depth-1">

	      	<div id="li-comment-587506" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/395aa700cfa36ed8677eed45cfca5a47_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/395aa700cfa36ed8677eed45cfca5a47.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Eric Orr</span>
	                <span class="date">December 22, 2020 at 11:48 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-587506" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>After additional consideration, it appears that the second sentence might instead be related as follows.</p>
<p>“In probability distributions where the events are equally likely, no
 events have larger or smaller likelihood (smaller or larger surprise, 
respectively), and the distribution has larger entropy.”</p>
<p>Sorry for belaboring this.  It is a good point but sometimes confusing.</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-587506" data-commentid="587506" data-postid="8877" data-belowelement="comment-587506" data-respondelement="respond" data-replyto="Reply to Eric Orr" aria-label="Reply to Eric Orr">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->

		<li id="comment-592015" class="comment even thread-even depth-1 parent">

	      	<div id="li-comment-592015" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/d4e78f69e4840973b6dfdf2e3c9899e7_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/d4e78f69e4840973b6dfdf2e3c9899e7.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">stefani</span>
	                <span class="date">January 6, 2021 at 11:19 pm</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-592015" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>They say that you have understood a concept when you can describe
 it with very simple words and I feel that is the case here. Wonderful 
job!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-592015" data-commentid="592015" data-postid="8877" data-belowelement="comment-592015" data-respondelement="respond" data-replyto="Reply to stefani" aria-label="Reply to stefani">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	<ul class="children">

		<li id="comment-592060" class="comment byuser comment-author-jasonb bypostauthor odd alt depth-2">

	      	<div id="li-comment-592060" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a.png" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a0942b56b07831ac15d4a168a750e34a_002.png 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name"><a href="http://machinelearningmastery.com/" rel="external nofollow ugc" class="url">Jason Brownlee</a></span>
	                <span class="date">January 7, 2021 at 6:18 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-592060" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>Thanks!</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-592060" data-commentid="592060" data-postid="8877" data-belowelement="comment-592060" data-respondelement="respond" data-replyto="Reply to Jason Brownlee" aria-label="Reply to Jason Brownlee">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->

		<li id="comment-598112" class="comment even thread-odd thread-alt depth-1">

	      	<div id="li-comment-598112" class="comment-container">

					                <div class="avatar"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/fd31ec80478a31167be81b9e81add45c.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/fd31ec80478a31167be81b9e81add45c_002.jpeg 2x" class="avatar avatar-40 photo" loading="lazy" width="40" height="40"></div>
	            
		      	<div class="comment-head">

	                <span class="name">Aaron</span>
	                <span class="date">February 20, 2021 at 8:52 am</span>
	                <span class="perma"><a href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#comment-598112" title="Direct link to this comment">#</a></span>
	                <span class="edit"></span>

				</div><!-- /.comment-head -->

		   		<div class="comment-entry">

				<p>“Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P.”</p>
<p>Souldn’t it rather say: Relative Entropy (KL Divergence): Average 
number of extra bits to represent an event from P using Q instead of P.</p>
<p>Since the expectation is over P(x)*[…]</p>

				
	                <div class="reply">
	                    <a rel="nofollow" class="comment-reply-link" href="#comment-598112" data-commentid="598112" data-postid="8877" data-belowelement="comment-598112" data-respondelement="respond" data-replyto="Reply to Aaron" aria-label="Reply to Aaron">Reply</a>	                </div><!-- /.reply -->

				</div><!-- /comment-entry -->

			</div><!-- /.comment-container -->

	</li><!-- #comment-## -->
			</ol>
		 	</div>	<div id="respond" class="comment-respond">
		<h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/#respond" style="display:none;">Click here to cancel reply.</a></small></h3><form action="https://machinelearningmastery.com/wp-comments-post.php?wpe-comment-post=mlmastery" method="post" id="commentform" class="comment-form"><p class="comment-form-comment"><label class="hide" for="comment">Comment</label> <textarea tabindex="4" id="comment" name="comment" cols="50" rows="10" maxlength="65525" required="required"></textarea></p><p class="comment-form-author"><input id="author" name="author" type="text" class="txt" tabindex="1" size="30" aria-required="true"><label for="author">Name <span class="required">(required)</span></label> </p>
<p class="comment-form-email"><input id="email" name="email" type="text" class="txt" tabindex="2" size="30" aria-required="true"><label for="email">Email (will not be published) <span class="required">(required)</span></label> </p>
<p class="comment-form-url"><input id="url" name="url" type="text" class="txt" tabindex="3" size="30"><label for="url">Website</label></p>
<p class="form-submit"><input name="submit" type="submit" id="submit" class="submit" value="Submit Comment"> <input type="hidden" name="comment_post_ID" value="8877" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
</p><p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="070bf04a06"></p><textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100" style="display: none !important;"></textarea><input type="hidden" id="ak_js" name="ak_js" value="1616858927283"></form>	</div><!-- #respond -->
	     
            </section><!-- /#main -->
                
            <aside id="sidebar">
<div id="woo_blogauthorinfo-2" class="widget widget_woo_blogauthorinfo"><span class="left"><img alt="" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75d46040c28497f0dee5d8e100db37_002.jpeg" srcset="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/1d75d46040c28497f0dee5d8e100db37.jpeg 2x" class="avatar avatar-88 photo" loading="lazy" width="88" height="88"></span>
<p><strong>Welcome!</strong><br>
I'm <em>Jason Brownlee</em> PhD <br>
and I <strong>help developers</strong> get results with <strong>machine learning</strong>.<br>
<a href="https://machinelearningmastery.com/about">Read more</a></p>
<div class="fix"></div>
</div><div id="custom_html-89_clone" class="widget_text widget widget_custom_html q2w3-widget-clone-primary" style="top: 10px; width: 306.333px; height: 68.7px; visibility: hidden;"></div><div id="custom_html-89" class="widget_text widget widget_custom_html" style="top: 10px; width: 306.333px; position: fixed;"><div class="textwidget custom-html-widget"><h4>Never miss a tutorial:</h4>
<br>
<div style="text-align: left;">
<a href="https://www.linkedin.com/company/machine-learning-mastery/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/small_icon_blue_linkedin3.webp" alt="LinkedIn" width="30" height="30"></a>
&nbsp;&nbsp;&nbsp;
<a href="https://twitter.com/TeachTheMachine"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/small_icon_blue_twitter3.webp" alt="Twitter" width="30" height="30"></a>
&nbsp;&nbsp;&nbsp;
<a href="https://www.facebook.com/MachineLearningMastery/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/small_icon_blue_facebook3.webp" alt="Facebook" width="30" height="30"></a>
&nbsp;&nbsp;&nbsp;
<a href="https://machinelearningmastery.com/newsletter/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/small_icon_blue_email3.webp" alt="Email Newsletter" width="30" height="30"></a>
&nbsp;&nbsp;&nbsp;
<a href="https://machinelearningmastery.com/rss-feed/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/small_icon_blue_rss3.webp" alt="RSS Feed" width="30" height="30"></a>
</div></div></div><div id="custom_html-59_clone" class="widget_text widget widget_custom_html q2w3-widget-clone-primary" style="top: 108.7px; width: 306.333px; height: 408.7px; visibility: hidden;"></div><div id="custom_html-59" class="widget_text widget widget_custom_html" style="top: 108.7px; width: 306.333px; position: fixed;"><div class="textwidget custom-html-widget"><h4>Picked for you:</h4>
<br>
<div class="display-posts-listing image-left"><div class="listing-item"><a class="image" href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/ROC-Curve-Plot-for-a-No-Skill-Classifier-and-a-Logistic-Reg.webp" class="attachment-thumbnail size-thumbnail wp-post-image" alt="ROC Curve Plot for a No Skill Classifier and a Logistic Regression Model" loading="lazy" width="150" height="150"></a> <a class="title" href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/" rel="nofollow">How to Use ROC Curves and Precision-Recall Curves for Classification in Python</a></div><div class="listing-item"><a class="image" href="https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Calibrated-and-Uncalibrated-SVM-Reliability-Diagram-150x150.webp" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Calibrated and Uncalibrated SVM Reliability Diagram" loading="lazy" width="150" height="150"></a> <a class="title" href="https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/" rel="nofollow">How and When to Use a Calibrated Classification Model with scikit-learn</a></div><div class="listing-item"><a class="image" href="https://machinelearningmastery.com/divergence-between-probability-distributions/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Histogram-of-Two-Different-Probability-Distributions-for-th.webp" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Histogram of Two Different Probability Distributions for the Same Random Variable" loading="lazy" width="150" height="150"></a> <a class="title" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="nofollow">How to Calculate the KL Divergence for Machine Learning</a></div><div class="listing-item"><a class="image" href="https://machinelearningmastery.com/what-is-bayesian-optimization/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Plot-of-The-Input-Samples-Evaluated-with-a-Noisy-dots-and-N.webp" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Plot of The Input Samples Evaluated with a Noisy (dots) and Non-Noisy (Line) Objective Function" loading="lazy" width="150" height="150"></a> <a class="title" href="https://machinelearningmastery.com/what-is-bayesian-optimization/" rel="nofollow">How to Implement Bayesian Optimization from Scratch in Python</a></div><div class="listing-item"><a class="image" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/"><img src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/Line-Plot-of-Probability-Distribution-vs-Cross-Entropy-for-.webp" class="attachment-thumbnail size-thumbnail wp-post-image" alt="Line Plot of Probability Distribution vs Cross-Entropy for a Binary Classification Task With Extreme Case Removed" loading="lazy" width="150" height="150"></a> <a class="title" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="nofollow">A Gentle Introduction to Cross-Entropy for Machine Learning</a></div></div></div></div><div id="custom_html-61_clone" class="widget_text widget widget_custom_html q2w3-widget-clone-primary" style="top: 547.4px; width: 306.333px; height: 139.233px; visibility: hidden;"></div><div id="custom_html-61" class="widget_text widget widget_custom_html" style="top: 547.4px; width: 306.333px; position: fixed;"><div class="textwidget custom-html-widget"><div style="text-align: center;">
	<p></p><h4>Loving the Tutorials?</h4><p></p>

<p>The <a href="https://machinelearningmastery.com/probability-for-machine-learning/" rel="nofollow">Probability for Machine Learning</a> EBook is where you'll find the <strong><i>Really Good</i></strong> stuff.</p>

<a href="https://machinelearningmastery.com/probability-for-machine-learning/" class="woo-sc-button  red"><span class="woo-">&gt;&gt; See What's Inside</span></a>

</div></div></div></aside><!-- /#sidebar -->

		</div><!-- /#main-sidebar-container -->         

		
    </div><!-- /#content -->
	
	<footer id="footer" class="col-full">

		
		<div id="copyright" class="col-left">
			<p>© 2021 Machine Learning Mastery Pty. Ltd. All Rights Reserved.<br>
<a href="https://www.linkedin.com/company/machine-learning-mastery/">LinkedIn</a> |
<a href="https://twitter.com/TeachTheMachine">Twitter</a> |
<a href="https://www.facebook.com/MachineLearningMastery/">Facebook</a> |
<a href="https://machinelearningmastery.com/newsletter/">Newsletter</a> |
<a href="https://machinelearningmastery.com/rss-feed/">RSS</a></p>		</div>

		<div id="credit" class="col-right">
			<p><a href="https://machinelearningmastery.com/privacy/">Privacy</a> | 
<a href="https://machinelearningmastery.com/disclaimer/">Disclaimer</a> | 
<a href="https://machinelearningmastery.com/terms-of-service/">Terms</a> | 
<a href="https://machinelearningmastery.com/contact/">Contact</a> |
<a href="https://machinelearningmastery.com/sitemap/">Sitemap</a> |
<a href="https://machinelearningmastery.com/site-search/">Search</a></p>		</div>

	</footer>

	
	</div><!-- /#inner-wrapper -->

</div><!-- /#wrapper -->

<div class="fix"></div><!--/.fix-->

<!-- Drip -->
<script data-cfasync="false" type="text/javascript">
  var _dcq = _dcq || [];
  var _dcs = _dcs || {};
  _dcs.account = '9556588';

  (function() {
    var dc = document.createElement('script');
    dc.type = 'text/javascript'; dc.async = true;
    dc.src = '//tag.getdrip.com/9556588.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(dc, s);
  })();
</script>
<!-- end Drip -->
<script type="text/javascript" id="rocket-browser-checker-js-after">
"use strict";var _createClass=function(){function defineProperties(target,props){for(var i=0;i<props.length;i++){var descriptor=props[i];descriptor.enumerable=descriptor.enumerable||!1,descriptor.configurable=!0,"value"in descriptor&&(descriptor.writable=!0),Object.defineProperty(target,descriptor.key,descriptor)}}return function(Constructor,protoProps,staticProps){return protoProps&&defineProperties(Constructor.prototype,protoProps),staticProps&&defineProperties(Constructor,staticProps),Constructor}}();function _classCallCheck(instance,Constructor){if(!(instance instanceof Constructor))throw new TypeError("Cannot call a class as a function")}var RocketBrowserCompatibilityChecker=function(){function RocketBrowserCompatibilityChecker(options){_classCallCheck(this,RocketBrowserCompatibilityChecker),this.passiveSupported=!1,this._checkPassiveOption(this),this.options=!!this.passiveSupported&&options}return _createClass(RocketBrowserCompatibilityChecker,[{key:"_checkPassiveOption",value:function(self){try{var options={get passive(){return!(self.passiveSupported=!0)}};window.addEventListener("test",null,options),window.removeEventListener("test",null,options)}catch(err){self.passiveSupported=!1}}},{key:"initRequestIdleCallback",value:function(){!1 in window&&(window.requestIdleCallback=function(cb){var start=Date.now();return setTimeout(function(){cb({didTimeout:!1,timeRemaining:function(){return Math.max(0,50-(Date.now()-start))}})},1)}),!1 in window&&(window.cancelIdleCallback=function(id){return clearTimeout(id)})}},{key:"isDataSaverModeOn",value:function(){return"connection"in navigator&&!0===navigator.connection.saveData}},{key:"supportsLinkPrefetch",value:function(){var elem=document.createElement("link");return elem.relList&&elem.relList.supports&&elem.relList.supports("prefetch")&&window.IntersectionObserver&&"isIntersecting"in IntersectionObserverEntry.prototype}},{key:"isSlowConnection",value:function(){return"connection"in navigator&&"effectiveType"in navigator.connection&&("2g"===navigator.connection.effectiveType||"slow-2g"===navigator.connection.effectiveType)}}]),RocketBrowserCompatibilityChecker}();
</script>
<script type="text/javascript" id="rocket-delay-js-js-after">
(function() {
"use strict";var e=function(){function n(e,t){for(var r=0;r<t.length;r++){var n=t[r];n.enumerable=n.enumerable||!1,n.configurable=!0,"value"in n&&(n.writable=!0),Object.defineProperty(e,n.key,n)}}return function(e,t,r){return t&&n(e.prototype,t),r&&n(e,r),e}}();function n(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}var t=function(){function r(e,t){n(this,r),this.attrName="data-rocketlazyloadscript",this.browser=t,this.options=this.browser.options,this.triggerEvents=e,this.userEventListener=this.triggerListener.bind(this)}return e(r,[{key:"init",value:function(){this._addEventListener(this)}},{key:"reset",value:function(){this._removeEventListener(this)}},{key:"_addEventListener",value:function(t){this.triggerEvents.forEach(function(e){return window.addEventListener(e,t.userEventListener,t.options)})}},{key:"_removeEventListener",value:function(t){this.triggerEvents.forEach(function(e){return window.removeEventListener(e,t.userEventListener,t.options)})}},{key:"_loadScriptSrc",value:function(){var r=this,e=document.querySelectorAll("script["+this.attrName+"]");0!==e.length&&Array.prototype.slice.call(e).forEach(function(e){var t=e.getAttribute(r.attrName);e.setAttribute("src",t),e.removeAttribute(r.attrName)}),this.reset()}},{key:"triggerListener",value:function(){this._loadScriptSrc(),this._removeEventListener(this)}}],[{key:"run",value:function(){RocketBrowserCompatibilityChecker&&new r(["keydown","mouseover","touchmove","touchstart","wheel"],new RocketBrowserCompatibilityChecker({passive:!0})).init()}}]),r}();t.run();
}());
</script>
<script type="text/javascript" id="rocket-preload-links-js-extra">
/* <![CDATA[ */
var RocketPreloadLinksConfig = {"excludeUris":"\/(.+\/)?feed\/?.+\/?|\/(?:.+\/)?embed\/|\/(index\\.php\/)?wp\\-json(\/.*|$)|\/wp-admin\/|\/logout\/|\/wp-login.php","usesTrailingSlash":"1","imageExt":"jpg|jpeg|gif|png|tiff|bmp|webp|avif","fileExt":"jpg|jpeg|gif|png|tiff|bmp|webp|avif|php|pdf|html|htm","siteUrl":"https:\/\/machinelearningmastery.com","onHoverDelay":"100","rateThrottle":"3"};
/* ]]> */
</script>
<script type="text/javascript" id="rocket-preload-links-js-after">
(function() {
"use strict";var r="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},e=function(){function i(e,t){for(var n=0;n<t.length;n++){var i=t[n];i.enumerable=i.enumerable||!1,i.configurable=!0,"value"in i&&(i.writable=!0),Object.defineProperty(e,i.key,i)}}return function(e,t,n){return t&&i(e.prototype,t),n&&i(e,n),e}}();function i(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}var t=function(){function n(e,t){i(this,n),this.browser=e,this.config=t,this.options=this.browser.options,this.prefetched=new Set,this.eventTime=null,this.threshold=1111,this.numOnHover=0}return e(n,[{key:"init",value:function(){!this.browser.supportsLinkPrefetch()||this.browser.isDataSaverModeOn()||this.browser.isSlowConnection()||(this.regex={excludeUris:RegExp(this.config.excludeUris,"i"),images:RegExp(".("+this.config.imageExt+")$","i"),fileExt:RegExp(".("+this.config.fileExt+")$","i")},this._initListeners(this))}},{key:"_initListeners",value:function(e){-1<this.config.onHoverDelay&&document.addEventListener("mouseover",e.listener.bind(e),e.listenerOptions),document.addEventListener("mousedown",e.listener.bind(e),e.listenerOptions),document.addEventListener("touchstart",e.listener.bind(e),e.listenerOptions)}},{key:"listener",value:function(e){var t=e.target.closest("a"),n=this._prepareUrl(t);if(null!==n)switch(e.type){case"mousedown":case"touchstart":this._addPrefetchLink(n);break;case"mouseover":this._earlyPrefetch(t,n,"mouseout")}}},{key:"_earlyPrefetch",value:function(t,e,n){var i=this,r=setTimeout(function(){if(r=null,0===i.numOnHover)setTimeout(function(){return i.numOnHover=0},1e3);else if(i.numOnHover>i.config.rateThrottle)return;i.numOnHover++,i._addPrefetchLink(e)},this.config.onHoverDelay);t.addEventListener(n,function e(){t.removeEventListener(n,e,{passive:!0}),null!==r&&(clearTimeout(r),r=null)},{passive:!0})}},{key:"_addPrefetchLink",value:function(i){return this.prefetched.add(i.href),new Promise(function(e,t){var n=document.createElement("link");n.rel="prefetch",n.href=i.href,n.onload=e,n.onerror=t,document.head.appendChild(n)}).catch(function(){})}},{key:"_prepareUrl",value:function(e){if(null===e||"object"!==(void 0===e?"undefined":r(e))||!1 in e||-1===["http:","https:"].indexOf(e.protocol))return null;var t=e.href.substring(0,this.config.siteUrl.length),n=this._getPathname(e.href,t),i={original:e.href,protocol:e.protocol,origin:t,pathname:n,href:t+n};return this._isLinkOk(i)?i:null}},{key:"_getPathname",value:function(e,t){var n=t?e.substring(this.config.siteUrl.length):e;return n.startsWith("/")||(n="/"+n),this._shouldAddTrailingSlash(n)?n+"/":n}},{key:"_shouldAddTrailingSlash",value:function(e){return this.config.usesTrailingSlash&&!e.endsWith("/")&&!this.regex.fileExt.test(e)}},{key:"_isLinkOk",value:function(e){return null!==e&&"object"===(void 0===e?"undefined":r(e))&&(!this.prefetched.has(e.href)&&e.origin===this.config.siteUrl&&-1===e.href.indexOf("?")&&-1===e.href.indexOf("#")&&!this.regex.excludeUris.test(e.href)&&!this.regex.images.test(e.href))}}],[{key:"run",value:function(){"undefined"!=typeof RocketPreloadLinksConfig&&new n(new RocketBrowserCompatibilityChecker({capture:!0,passive:!0}),RocketPreloadLinksConfig).init()}}]),n}();t.run();
}());
</script>



<script type="text/javascript">window.lazyLoadOptions={elements_selector:"iframe[data-lazy-src]",data_src:"lazy-src",data_srcset:"lazy-srcset",data_sizes:"lazy-sizes",class_loading:"lazyloading",class_loaded:"lazyloaded",threshold:300,callback_loaded:function(element){if(element.tagName==="IFRAME"&&element.dataset.rocketLazyload=="fitvidscompatible"){if(element.classList.contains("lazyloaded")){if(typeof window.jQuery!="undefined"){if(jQuery.fn.fitVids){jQuery(element).parent().fitVids()}}}}}};window.addEventListener('LazyLoad::Initialized',function(e){var lazyLoadInstance=e.detail.instance;if(window.MutationObserver){var observer=new MutationObserver(function(mutations){var image_count=0;var iframe_count=0;var rocketlazy_count=0;mutations.forEach(function(mutation){for(i=0;i<mutation.addedNodes.length;i++){if(typeof mutation.addedNodes[i].getElementsByTagName!=='function'){continue}
if(typeof mutation.addedNodes[i].getElementsByClassName!=='function'){continue}
images=mutation.addedNodes[i].getElementsByTagName('img');is_image=mutation.addedNodes[i].tagName=="IMG";iframes=mutation.addedNodes[i].getElementsByTagName('iframe');is_iframe=mutation.addedNodes[i].tagName=="IFRAME";rocket_lazy=mutation.addedNodes[i].getElementsByClassName('rocket-lazyload');image_count+=images.length;iframe_count+=iframes.length;rocketlazy_count+=rocket_lazy.length;if(is_image){image_count+=1}
if(is_iframe){iframe_count+=1}}});if(image_count>0||iframe_count>0||rocketlazy_count>0){lazyLoadInstance.update()}});var b=document.getElementsByTagName("body")[0];var config={childList:!0,subtree:!0};observer.observe(b,config)}},!1)</script><script async="" data-no-minify="1" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/lazyload.js" type="text/javascript"></script><script src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/b473ab520a9bde0ca73433f7d50abbad.js" data-minify="1" defer="defer" type="text/javascript"></script>

<div style="position: absolute; inset: 0px; z-index: 2147483646; background: black none repeat scroll 0% 0%; width: 100%; opacity: 0.5; display: none; height: 34370px;"></div><iframe src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/a.html" style="position: absolute; left: 0px; top: 0px; width: 100%; display: none; border: medium none; z-index: 2147483647; height: 722px;"></iframe><iframe scrolling="no" allowtransparency="true" src="A%20Gentle%20Introduction%20to%20Cross-Entropy%20for%20Machine%20Learning_files/widget_iframe.html" title="Twitter settings iframe" style="display: none;" frameborder="0"></iframe><iframe id="rufous-sandbox" scrolling="no" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: medium none;" title="Twitter analytics iframe" frameborder="0"></iframe></body></html>
<!-- This website is like a Rocket, isn't it? Performance optimized by WP Rocket. Learn more: https://wp-rocket.me -->